{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89d14750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Installing: python-slugify\n",
      "✓ openpyxl already installed\n",
      "✓ pyarrow already installed\n",
      "→ Installing: statsmodels\n",
      "→ Installing: pmdarima\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'patsy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m         subprocess\u001b[38;5;241m.\u001b[39mcheck_call([sys\u001b[38;5;241m.\u001b[39mexecutable, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-m\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpip\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstall\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--quiet\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--no-deps\u001b[39m\u001b[38;5;124m\"\u001b[39m, pip_name])\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Version check (info only)\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mopenpyxl\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mstatsmodels\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpmdarima\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mslugify\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mVERSIONS:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     22\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mpandas   :\u001b[39m\u001b[38;5;124m\"\u001b[39m, pd\u001b[38;5;241m.\u001b[39m__version__,\n\u001b[0;32m     23\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mpyarrow  :\u001b[39m\u001b[38;5;124m\"\u001b[39m, pyarrow\u001b[38;5;241m.\u001b[39m__version__,\n\u001b[0;32m     24\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mopenpyxl :\u001b[39m\u001b[38;5;124m\"\u001b[39m, openpyxl\u001b[38;5;241m.\u001b[39m__version__,\n\u001b[0;32m     25\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mstatsmodels:\u001b[39m\u001b[38;5;124m\"\u001b[39m, statsmodels\u001b[38;5;241m.\u001b[39m__version__,\n\u001b[0;32m     26\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mpmdarima :\u001b[39m\u001b[38;5;124m\"\u001b[39m, pmdarima\u001b[38;5;241m.\u001b[39m__version__)\n",
      "File \u001b[1;32mc:\\Users\\Selin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\statsmodels\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mstatsmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpatsy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m monkey_patch_cat_dtype\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mstatsmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__, __version_tuple__\n\u001b[0;32m      5\u001b[0m __version_info__ \u001b[38;5;241m=\u001b[39m __version_tuple__\n",
      "File \u001b[1;32mc:\\Users\\Selin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\statsmodels\\compat\\patsy.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mstatsmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PD_LT_2\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpatsy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_safe_is_pandas_categorical_dtype\u001b[39m(dt):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m PD_LT_2:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'patsy'"
     ]
    }
   ],
   "source": [
    "import importlib, sys, subprocess\n",
    "\n",
    "need = {\n",
    "    \"python-slugify\": \"slugify\",\n",
    "    \"openpyxl\": \"openpyxl\",\n",
    "    \"pyarrow\": \"pyarrow\",\n",
    "    \"statsmodels\": \"statsmodels\",\n",
    "    \"pmdarima\": \"pmdarima\"\n",
    "}\n",
    "\n",
    "for pip_name, mod_name in need.items():\n",
    "    try:\n",
    "        importlib.import_module(mod_name)\n",
    "        print(f\"✓ {pip_name} already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"→ Installing: {pip_name}\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", \"--no-deps\", pip_name])\n",
    "\n",
    "# Version check (info only)\n",
    "import pandas as pd, pyarrow, openpyxl, statsmodels, pmdarima, slugify\n",
    "print(\"\\nVERSIONS:\",\n",
    "      \"\\npandas   :\", pd.__version__,\n",
    "      \"\\npyarrow  :\", pyarrow.__version__,\n",
    "      \"\\nopenpyxl :\", openpyxl.__version__,\n",
    "      \"\\nstatsmodels:\", statsmodels.__version__,\n",
    "      \"\\npmdarima :\", pmdarima.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1ee8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 / Cell 2: Libraries and helper functions\n",
    "from pathlib import Path\n",
    "import json, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from slugify import slugify\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 50)\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "\n",
    "# Kaggle directories\n",
    "SEARCH_DIRS = [Path(\"/kaggle/working\"), Path(\"/kaggle/input\")]\n",
    "\n",
    "def find_file_by_keywords(keywords, exts=(\"xlsx\",\"xls\",\"csv\")):\n",
    "    \"\"\"\n",
    "    keywords: substrings like ['scratch']\n",
    "    return: the first found file path (we choose the one with the shortest name)\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "    for base in SEARCH_DIRS:\n",
    "        if not base.exists(): \n",
    "            continue\n",
    "        for ext in exts:\n",
    "            for p in base.rglob(f\"*.{ext}\"):\n",
    "                name_lower = p.name.lower()\n",
    "                if all(k.lower() in name_lower for k in keywords):\n",
    "                    candidates.append(p)\n",
    "    if not candidates:\n",
    "        return None\n",
    "    # pick the 'cleaner' (shorter) name\n",
    "    candidates = sorted(candidates, key=lambda p: (len(p.name), str(p)))\n",
    "    return candidates[0]\n",
    "\n",
    "def normalize_columns(df):\n",
    "    \"\"\"\n",
    "    Simplifies Turkish characters and spaces.\n",
    "    Eg: 'Yeni Kullanıcı Sayısı' -> 'yeni_kullanici_sayisi'\n",
    "    \"\"\"\n",
    "    mapping = {}\n",
    "    for c in df.columns:\n",
    "        # slugify: converts Turkish characters to Latin, turns spaces into '-'\n",
    "        s = slugify(str(c), separator=\"_\")\n",
    "        s = re.sub(r\"[^a-z0-9_]+\", \"\", s)\n",
    "        s = re.sub(r\"__+\", \"_\", s).strip(\"_\")\n",
    "        mapping[c] = s\n",
    "    return df.rename(columns=mapping)\n",
    "\n",
    "def to_numeric_safe(series: pd.Series):\n",
    "    \"\"\"\n",
    "    Safely converts strings that look numeric to numeric.\n",
    "    - '1.234,56' -> 1234.56\n",
    "    - '1,234.56' -> 1234.56\n",
    "    - '1.234' or '1,234' -> assume 1234 (thousands separator)\n",
    "    \"\"\"\n",
    "    if series.dtype.kind in \"biufc\":\n",
    "        return series  # already numeric\n",
    "\n",
    "    s = series.astype(str).str.strip().replace({\"\": np.nan})\n",
    "    # keep only numeric/separator characters and sign/space\n",
    "    s_norm = s.str.replace(r\"[^\\d\\.,\\-+eE ]\", \"\", regex=True)\n",
    "\n",
    "    def _convert(val):\n",
    "        if pd.isna(val): \n",
    "            return np.nan\n",
    "        text = str(val).strip()\n",
    "        if text.count(\",\") > 0 and text.count(\".\") > 0:\n",
    "            # type '1.234,56' -> . thousands, , decimal\n",
    "            text = text.replace(\".\", \"\").replace(\",\", \".\")\n",
    "        elif \",\" in text and \".\" not in text:\n",
    "            # '1234,56' -> ',' decimal\n",
    "            # but '1,234' could be thousands; heuristic: if comma is 3 from the end, it's thousands\n",
    "            parts = text.split(\",\")\n",
    "            if len(parts[-1]) in (1,2):  # most likely decimal\n",
    "                text = text.replace(\",\", \".\")\n",
    "            else:\n",
    "                text = text.replace(\",\", \"\")\n",
    "        else:\n",
    "            # '1.234' or '1234.56' -> either thousands or decimal\n",
    "            parts = text.split(\".\")\n",
    "            if len(parts) > 1 and len(parts[-1]) not in (1,2):\n",
    "                text = text.replace(\".\", \"\")\n",
    "        try:\n",
    "            return float(text)\n",
    "        except:\n",
    "            return np.nan\n",
    "\n",
    "    return s_norm.apply(_convert)\n",
    "\n",
    "def infer_datetime(df):\n",
    "    \"\"\"\n",
    "    Date column inference:\n",
    "    - a single column like 'date'/'tarih'\n",
    "    - year + month (+day?) combination\n",
    "    - year + week combination (assumes Monday as week start)\n",
    "    returns: (df_copy, name of the 'datetime' column or None, inferred_frequency)\n",
    "    \"\"\"\n",
    "    d = df.copy()\n",
    "    cols = list(d.columns)\n",
    "\n",
    "    # candidates\n",
    "    date_like = [c for c in cols if re.search(r\"(date|tarih|week|ay|month|yil|year|hafta)\", c)]\n",
    "    date_col = None\n",
    "    dt = None\n",
    "\n",
    "    # priority 1: a single 'date'/'tarih' column\n",
    "    for key in [\"date\",\"tarih\"]:\n",
    "        cands = [c for c in cols if c == key or c.endswith(\"_\"+key)]\n",
    "        if len(cands)==1:\n",
    "            date_col = cands[0]\n",
    "            break\n",
    "\n",
    "    # otherwise, a single column containing 'date|tarih'?\n",
    "    if not date_col:\n",
    "        cands = [c for c in cols if re.search(r\"(date|tarih)\", c)]\n",
    "        if len(cands)==1:\n",
    "            date_col = cands[0]\n",
    "\n",
    "    def _parse_datecol(col):\n",
    "        out = pd.to_datetime(d[col], errors=\"coerce\", dayfirst=True, infer_datetime_format=True)\n",
    "        return out\n",
    "\n",
    "    if date_col:\n",
    "        dt = _parse_datecol(date_col)\n",
    "        if dt.notna().any():\n",
    "            d[\"__dt__\"] = dt\n",
    "    else:\n",
    "        # year + month (+day)\n",
    "        year_cols  = [c for c in cols if re.search(r\"(yil|year)\", c)]\n",
    "        month_cols = [c for c in cols if re.search(r\"(ay|month)\", c)]\n",
    "        day_cols   = [c for c in cols if re.search(r\"(gun|day)\", c)]\n",
    "\n",
    "        if year_cols and month_cols:\n",
    "            y = pd.to_numeric(df[year_cols[0]], errors=\"coerce\")\n",
    "            m = pd.to_numeric(df[month_cols[0]], errors=\"coerce\")\n",
    "            dday = pd.Series(1, index=df.index)\n",
    "            if day_cols:\n",
    "                dday = pd.to_numeric(df[day_cols[0]], errors=\"coerce\").fillna(1)\n",
    "            d[\"__dt__\"] = pd.to_datetime(dict(year=y, month=m, day=dday), errors=\"coerce\")\n",
    "        else:\n",
    "            # year + week\n",
    "            week_cols = [c for c in cols if re.search(r\"(hafta|week)\", c)]\n",
    "            if year_cols and week_cols:\n",
    "                y = pd.to_numeric(df[year_cols[0]], errors=\"coerce\").astype(\"Int64\")\n",
    "                w = pd.to_numeric(df[week_cols[0]], errors=\"coerce\").astype(\"Int64\")\n",
    "                # ISO week assumption: Monday start\n",
    "                # (week 1 may start at the end of the previous year; simple approach:)\n",
    "                d[\"__dt__\"] = pd.to_datetime(\n",
    "                    y.astype(str) + \"-W\" + w.astype(str) + \"-1\", errors=\"coerce\", format=\"%G-W%V-%u\"\n",
    "                )\n",
    "\n",
    "    freq_guess = None\n",
    "    if \"__dt__\" in d.columns and d[\"__dt__\"].notna().sum() >= 3:\n",
    "        # for frequency inference, sort and take unique values\n",
    "        s = d[\"__dt__\"].dropna().sort_values().unique()\n",
    "        if len(s) >= 3:\n",
    "            try:\n",
    "                freq_guess = pd.infer_freq(pd.DatetimeIndex(s))\n",
    "            except:\n",
    "                freq_guess = None\n",
    "\n",
    "    return d, (\"__dt__\" if \"__dt__\" in d.columns else None), freq_guess\n",
    "\n",
    "def quick_report(name, df, dt_col=None):\n",
    "    print(f\"\\n===== DATA INTAKE SUMMARY :: {name} =====\")\n",
    "    print(\"Shape:\", df.shape)\n",
    "    print(\"Columns:\", list(df.columns))\n",
    "    if dt_col and dt_col in df.columns:\n",
    "        dd = df[dt_col].dropna().sort_values()\n",
    "        if not dd.empty:\n",
    "            print(\"Date range:\", dd.iloc[0].date(), \"→\", dd.iloc[-1].date())\n",
    "            try:\n",
    "                print(\"Unique periods:\", dd.nunique())\n",
    "            except:\n",
    "                pass\n",
    "    # missing value report\n",
    "    na = df.isna().mean().sort_values(ascending=False)\n",
    "    print(\"\\nNull ratios (top 10):\")\n",
    "    print((na.head(10)*100).round(2).astype(str) + \"%\")\n",
    "    # numeric detection\n",
    "    num_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
    "    print(\"\\nNumeric columns:\", num_cols[:20])\n",
    "    print(\"Sample rows:\\n\", df.head(5))\n",
    "\n",
    "def make_schema(df, name):\n",
    "    schema = {}\n",
    "    for c in df.columns:\n",
    "        dtype = str(df[c].dtype)\n",
    "        null_ratio = float(df[c].isna().mean())\n",
    "        schema[c] = {\"dtype\": dtype, \"null_ratio\": null_ratio}\n",
    "    out = {\n",
    "        \"dataset\": name,\n",
    "        \"n_rows\": int(len(df)),\n",
    "        \"n_cols\": int(df.shape[1]),\n",
    "        \"schema\": schema,\n",
    "    }\n",
    "    return out\n",
    "\n",
    "# output directory\n",
    "OUT = Path(\"/kaggle/working/clean\")\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd68241f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 / Cell 3: Auto-detect file paths\n",
    "scratch_path = find_file_by_keywords([\"scratch\"])\n",
    "trends_path  = find_file_by_keywords([\"google\",\"trend\"]) or find_file_by_keywords([\"kodlama\",\"ilgi\"]) \n",
    "\n",
    "print(\"Scratch file:\", scratch_path)\n",
    "print(\"Trends  file:\", trends_path)\n",
    "\n",
    "assert scratch_path is not None, \"Scratch data not found. Ensure the filename contains 'scratch' or provide the path manually in Cell 3.\"\n",
    "assert trends_path  is not None, \"Google Trends data not found. Ensure the filename contains 'google' and 'trend' (or provide the path manually in Cell 3).\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a63449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 / Cell 4C (Rev2): Read Google Trends with a two-row header, preserve category & term (long form, safe)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def _fill_and_slugify_multicol(multi_cols):\n",
    "    lvl0 = pd.Series(multi_cols.get_level_values(0), dtype=\"object\")\n",
    "    lvl1 = pd.Series(multi_cols.get_level_values(1), dtype=\"object\")\n",
    "    lvl0 = lvl0.replace({None: np.nan, \"\": np.nan}).ffill()  # forward-fill category headers to the right\n",
    "    cat = lvl0.apply(lambda x: slugify(str(x), separator=\"_\"))\n",
    "    sub = lvl1.apply(lambda x: slugify(str(x), separator=\"_\"))\n",
    "    return pd.MultiIndex.from_arrays([cat, sub], names=[\"category\", \"term\"])\n",
    "\n",
    "def read_trends_excel_with_categories(path: Path):\n",
    "    df = pd.read_excel(path, sheet_name=0, header=[0, 1], engine=\"openpyxl\")\n",
    "    df = df.dropna(how=\"all\").dropna(how=\"all\", axis=1)\n",
    "    df.columns = _fill_and_slugify_multicol(df.columns)\n",
    "\n",
    "    # year & month columns: regardless of category, terms named 'yil'/'ay'\n",
    "    y_candidates = [c for c in df.columns if c[1] in (\"yil\",\"year\")]\n",
    "    m_candidates = [c for c in df.columns if c[1] in (\"ay\",\"month\")]\n",
    "    assert y_candidates and m_candidates, \"Google Trends: 'year' and 'month' columns not found.\"\n",
    "    ycol, mcol = y_candidates[0], m_candidates[0]\n",
    "\n",
    "    # date\n",
    "    date = pd.to_datetime(\n",
    "        dict(\n",
    "            year=pd.to_numeric(df[ycol], errors=\"coerce\"),\n",
    "            month=pd.to_numeric(df[mcol], errors=\"coerce\"),\n",
    "            day=1\n",
    "        ),\n",
    "        errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "    # metric columns (excluding year/month)\n",
    "    metric_cols = [c for c in df.columns if c not in (ycol, mcol) and c[1] not in (\"yil\",\"year\",\"ay\",\"month\")]\n",
    "\n",
    "    # take only metrics, convert to numeric\n",
    "    metrics = df.loc[:, metric_cols].copy()\n",
    "    for c in metric_cols:\n",
    "        metrics[c] = to_numeric_safe(metrics[c])\n",
    "    # ensure MultiIndex names\n",
    "    metrics.columns = pd.MultiIndex.from_tuples(metric_cols, names=[\"category\",\"term\"])\n",
    "\n",
    "    # long form: stack only metrics, add date at the end\n",
    "    long = (\n",
    "        metrics\n",
    "        .assign(date=date)\n",
    "        .set_index(\"date\")\n",
    "        .stack([\"category\",\"term\"])\n",
    "        .reset_index()\n",
    "        .rename(columns={0: \"interest\"})\n",
    "    )\n",
    "    # column order and cleanup\n",
    "    long = long[[\"date\",\"category\",\"term\",\"interest\"]]\n",
    "    long = long.dropna(subset=[\"date\"]).sort_values([\"category\",\"term\",\"date\"]).reset_index(drop=True)\n",
    "    long.loc[(long[\"interest\"] < 0) | (long[\"interest\"] > 100), \"interest\"] = np.nan\n",
    "\n",
    "    return long\n",
    "\n",
    "trends_long = read_trends_excel_with_categories(trends_path)\n",
    "quick_report(\"Google Trends (long, with categories)\", trends_long, dt_col=\"date\")\n",
    "\n",
    "# Save\n",
    "trends_long.to_parquet(OUT / \"google_trends_long.parquet\", index=False)\n",
    "trends_long.to_csv(OUT / \"google_trends_long.csv\", index=False, encoding=\"utf-8\")\n",
    "with open(OUT / \"google_trends_schema.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(make_schema(trends_long, \"google_trends_long\"), f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"\\nSaved:\", OUT / \"google_trends_long.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55127a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 / Cell 5C (Rev2): Read Scratch with a two-row header, preserve category & metric (wide + long, safe)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def read_scratch_excel_with_categories_v2(path: Path):\n",
    "    # Read with multi-row header\n",
    "    df = pd.read_excel(path, sheet_name=0, header=[0, 1], engine=\"openpyxl\")\n",
    "    df = df.dropna(how=\"all\").dropna(how=\"all\", axis=1)\n",
    "    df.columns = _fill_and_slugify_multicol(df.columns)  # names=[\"category\",\"metric\"]\n",
    "\n",
    "    # year & month columns (terms named 'yil'/'ay')\n",
    "    y_candidates = [c for c in df.columns if c[1] in (\"yil\",\"year\")]\n",
    "    m_candidates = [c for c in df.columns if c[1] in (\"ay\",\"month\")]\n",
    "    assert y_candidates and m_candidates, \"Scratch: 'year' and 'month' columns not found.\"\n",
    "    ycol, mcol = y_candidates[0], m_candidates[0]\n",
    "\n",
    "    # Date series\n",
    "    date = pd.to_datetime(\n",
    "        dict(\n",
    "            year=pd.to_numeric(df[ycol], errors=\"coerce\"),\n",
    "            month=pd.to_numeric(df[mcol], errors=\"coerce\"),\n",
    "            day=1\n",
    "        ),\n",
    "        errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "    # Metric columns: all pairs except year/month\n",
    "    metric_cols = [c for c in df.columns if c not in (ycol, mcol) and c[1] not in (\"yil\",\"year\",\"ay\",\"month\")]\n",
    "\n",
    "    # Copy only metrics and convert to numeric\n",
    "    metrics = df.loc[:, metric_cols].copy()\n",
    "    for c in metric_cols:\n",
    "        metrics[c] = to_numeric_safe(metrics[c])\n",
    "\n",
    "    # --- WIDE table (columns with simple names: new_projects, new_users, new_comments) ---\n",
    "    simple_names = [term for (cat, term) in metric_cols]  # use the lower name since the top category is single\n",
    "    wide = pd.DataFrame({\"date\": date})\n",
    "    for col, new_name in zip(metric_cols, simple_names):\n",
    "        wide[new_name] = metrics[col].values\n",
    "    wide = wide.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "    # --- LONG table (keep category) ---\n",
    "    # metrics columns are MultiIndex; stack and add 'date'\n",
    "    metrics.columns = pd.MultiIndex.from_tuples(metric_cols, names=[\"category\",\"metric\"])\n",
    "    long = (\n",
    "        metrics.assign(date=date)\n",
    "               .set_index(\"date\")\n",
    "               .stack([\"category\",\"metric\"])\n",
    "               .reset_index()\n",
    "               .rename(columns={0: \"value\"})\n",
    "               .sort_values([\"category\",\"metric\",\"date\"])\n",
    "               .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # Frequency inference\n",
    "    try:\n",
    "        scratch_freq = pd.infer_freq(pd.DatetimeIndex(wide[\"date\"].dropna().unique()))\n",
    "    except:\n",
    "        scratch_freq = None\n",
    "\n",
    "    return wide, long, \"date\", scratch_freq\n",
    "\n",
    "# Run\n",
    "scratch_df, scratch_long, scratch_dt, scratch_freq = read_scratch_excel_with_categories_v2(scratch_path)\n",
    "\n",
    "quick_report(\"Scratch Wide (with categories kept separately)\", scratch_df, dt_col=scratch_dt)\n",
    "print(\"Inferred frequency (Scratch):\", scratch_freq)\n",
    "\n",
    "# Save (wide + long)\n",
    "fname_base = \"scratch_clean\"\n",
    "scratch_df.to_parquet(OUT / f\"{fname_base}.parquet\", index=False)\n",
    "scratch_df.to_csv(OUT / f\"{fname_base}.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "scratch_long.to_parquet(OUT / f\"{fname_base}_long.parquet\", index=False)\n",
    "scratch_long.to_csv(OUT / f\"{fname_base}_long.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "with open(OUT / \"scratch_schema.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(make_schema(scratch_df, \"scratch_clean (wide)\"), f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"\\nSaved:\", OUT / f\"{fname_base}.parquet\", \"and\", OUT / f\"{fname_base}_long.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75cb273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 / Cell 6C: Categorized summary message\n",
    "\n",
    "from textwrap import dedent\n",
    "\n",
    "terms_preview = trends_long.groupby(\"category\")[\"term\"].nunique().reset_index().sort_values(\"term\", ascending=False)\n",
    "cat_list = trends_long[\"category\"].dropna().unique().tolist()\n",
    "\n",
    "msg = dedent(f\"\"\"\n",
    "\n",
    "[1] Google Trends (long, with categories) — Summary\n",
    "- shape: {trends_long.shape[0]} rows x {trends_long.shape[1]} cols\n",
    "- categories: {cat_list}\n",
    "- term counts (top 8):\n",
    "{terms_preview.head(8).to_string(index=False)}\n",
    "- date range: {trends_long['date'].min().date()} → {trends_long['date'].max().date()}\n",
    "- sample rows:\n",
    "{trends_long.head(8).to_string(index=False)[:900]}\n",
    "\n",
    "[2] Scratch — Summary\n",
    "- wide shape: {scratch_df.shape[0]} rows x {scratch_df.shape[1]} cols\n",
    "- long shape: {scratch_long.shape[0]} rows x {scratch_long.shape[1]} cols\n",
    "- date range: {scratch_df['date'].min().date()} → {scratch_df['date'].max().date()}\n",
    "- metrics (wide): {[c for c in scratch_df.columns if c!='date']}\n",
    "- inferred frequency (scratch): {scratch_freq}\n",
    "\n",
    "[3] Saved files (Kaggle /kaggle/working/clean):\n",
    "- google_trends_long.parquet / .csv  (columns: date, category, term, interest)\n",
    "- scratch_clean.parquet / .csv       (wide: date + metrics)\n",
    "- scratch_clean_long.parquet / .csv  (long: date, category, metric, value)\n",
    "\"\"\").strip()\n",
    "\n",
    "print(msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08780ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 / Cell A1: Helper functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "CLEAN = Path(\"/kaggle/working/clean\")\n",
    "\n",
    "def zfill_month(dt: pd.Series):\n",
    "    \"\"\"\n",
    "    Resets months to the 'first day of the month'.\n",
    "    Why? to_timestamp('MS') is deprecated; we use 'M' + how='start'.\n",
    "    \"\"\"\n",
    "    s = pd.to_datetime(dt, errors=\"coerce\")\n",
    "    return s.dt.to_period(\"M\").dt.to_timestamp(how=\"start\")\n",
    "\n",
    "\n",
    "def ma(s: pd.Series, k=3):\n",
    "    \"\"\"Simple moving average (default k=3).\"\"\"\n",
    "    return s.rolling(k, min_periods=1, center=False).mean()\n",
    "\n",
    "def minmax_0_100(s: pd.Series):\n",
    "    \"\"\"Min-max normalize to the 0–100 range (for direct comparison).\"\"\"\n",
    "    s = s.astype(float)\n",
    "    lo, hi = s.min(), s.max()\n",
    "    if pd.isna(lo) or pd.isna(hi) or hi == lo:\n",
    "        return pd.Series(np.nan, index=s.index)\n",
    "    return (s - lo) / (hi - lo) * 100.0\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 4)\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "plt.rcParams[\"figure.dpi\"] = 110\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0add532b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 / Cell A2: Load cleaned data\n",
    "trends_long = pd.read_parquet(CLEAN / \"google_trends_long.parquet\")\n",
    "scratch_wide = pd.read_parquet(CLEAN / \"scratch_clean.parquet\")  # date, new_projects, new_users, new_comments\n",
    "\n",
    "# Normalize dates (start of month)\n",
    "trends_long[\"date\"] = zfill_month(trends_long[\"date\"])\n",
    "scratch_wide[\"date\"] = zfill_month(scratch_wide[\"date\"])\n",
    "\n",
    "# Short summary\n",
    "print(\"Trends:\", trends_long.shape, trends_long[\"date\"].min().date(), \"→\", trends_long[\"date\"].max().date(),\n",
    "      \"| categories:\", trends_long[\"category\"].unique().tolist())\n",
    "print(\"Scratch:\", scratch_wide.shape, scratch_wide[\"date\"].min().date(), \"→\", scratch_wide[\"date\"].max().date())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2cdb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 / Cell A3: Summary tables (categories, terms, scratch metrics)\n",
    "\n",
    "# 1) Number of terms per category\n",
    "terms_per_cat = trends_long.groupby(\"category\")[\"term\"].nunique().sort_values(ascending=False)\n",
    "print(\"Term count / category:\\n\", terms_per_cat, \"\\n\")\n",
    "\n",
    "# 2) Average interest (0-100) for each term, top 10\n",
    "term_means = (trends_long.groupby([\"category\",\"term\"])[\"interest\"]\n",
    "              .mean().sort_values(ascending=False))\n",
    "print(\"Highest average interest (top 10):\\n\", term_means.head(10), \"\\n\")\n",
    "\n",
    "# 3) Scratch summary statistics\n",
    "scratch_desc = scratch_wide.describe()[[\"new_projects\",\"new_users\",\"new_comments\"]]\n",
    "print(\"Scratch metrics — summary statistics:\\n\", scratch_desc, \"\\n\")\n",
    "\n",
    "# 4) Intersection date range (for correlation later)\n",
    "start = max(trends_long[\"date\"].min(), scratch_wide[\"date\"].min())\n",
    "end   = min(trends_long[\"date\"].max(), scratch_wide[\"date\"].max())\n",
    "print(\"Intersection date range:\", start.date(), \"→\", end.date())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cfae3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 / Cell A4: Category averages (3M MA)\n",
    "\n",
    "# Average interest per month-category\n",
    "cat_monthly = (trends_long.groupby([\"date\",\"category\"])[\"interest\"]\n",
    "               .mean().reset_index())\n",
    "pivot_cat = cat_monthly.pivot(index=\"date\", columns=\"category\", values=\"interest\")\n",
    "\n",
    "# 3-month moving average\n",
    "pivot_cat_ma = pivot_cat.apply(lambda s: ma(s, 3))\n",
    "\n",
    "plt.figure()\n",
    "for c in pivot_cat_ma.columns:\n",
    "    plt.plot(pivot_cat_ma.index, pivot_cat_ma[c], label=c)\n",
    "plt.title(\"Google Trends — Category Average (3M MA)\")\n",
    "plt.xlabel(\"Date\"); plt.ylabel(\"Interest (0–100)\")\n",
    "plt.legend(loc=\"upper left\", ncol=2, fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678025ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 / Cell A5: Scratch metrics (3M MA)\n",
    "\n",
    "sw = scratch_wide.set_index(\"date\")[[\"new_projects\",\"new_users\",\"new_comments\"]]\n",
    "sw_ma = sw.apply(lambda s: ma(s, 3))\n",
    "\n",
    "plt.figure()\n",
    "for c in sw_ma.columns:\n",
    "    plt.plot(sw_ma.index, sw_ma[c], label=c)\n",
    "plt.title(\"Scratch — New Projects / Users / Comments (3M MA)\")\n",
    "plt.xlabel(\"Date\"); plt.ylabel(\"Count\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35896d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 / Cell A6: 'platformlar_araclar: scratch' vs 'new_users' (normalize → 0–100)\n",
    "\n",
    "# 1) From Google Trends, the 'scratch' term (in the platformlar_araclar category)\n",
    "scratch_term = (trends_long.query(\"category == 'platformlar_araclar' and term == 'scratch'\")\n",
    "                .set_index(\"date\")[\"interest\"].rename(\"trend_scratch\"))\n",
    "\n",
    "# 2) Scratch new users\n",
    "new_users = scratch_wide.set_index(\"date\")[\"new_users\"].rename(\"scratch_new_users\")\n",
    "\n",
    "# 3) Restrict to the common date range\n",
    "both = pd.concat([scratch_term, new_users], axis=1).dropna()\n",
    "\n",
    "# 4) 3M MA + normalize to 0–100\n",
    "both_ma = both.apply(lambda s: ma(s, 3))\n",
    "both_norm = both_ma.apply(minmax_0_100)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(both_norm.index, both_norm[\"trend_scratch\"], label=\"Trend: scratch (0–100, 3M MA)\")\n",
    "plt.plot(both_norm.index, both_norm[\"scratch_new_users\"], label=\"Scratch new_users (0–100, 3M MA)\")\n",
    "plt.title(\"Google Trends 'scratch' vs Scratch 'new_users' — normalized comparison\")\n",
    "plt.xlabel(\"Date\"); plt.ylabel(\"Normalized Score (0–100)\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b953128d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 / Cell A7: YoY change (annual %)\n",
    "\n",
    "# Scratch: YoY %\n",
    "sw = scratch_wide.set_index(\"date\")[[\"new_projects\",\"new_users\",\"new_comments\"]]\n",
    "scratch_yoy = (sw.pct_change(12) * 100).rename(columns=lambda c: c + \"_yoy_pct\")\n",
    "\n",
    "# Trends: category average YoY % (interest is 0–100, used to see direction of change)\n",
    "cat_y = (trends_long.groupby([\"date\",\"category\"])[\"interest\"].mean()\n",
    "         .unstack(\"category\"))\n",
    "trends_yoy = (cat_y.pct_change(12) * 100).add_suffix(\"_yoy_pct\")\n",
    "\n",
    "# Intersection and sample table\n",
    "yoy_merged = scratch_yoy.join(trends_yoy, how=\"inner\")\n",
    "print(\"YoY merged table shape:\", yoy_merged.shape)\n",
    "print(yoy_merged.tail(12).round(2).to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4f7cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 / Cell A8: Save the merged dataset (we’ll use it for correlation and lag analysis)\n",
    "\n",
    "# Add some selected terms as well (example: coding_for_kids, code_org, blockly)\n",
    "sel_terms = [\n",
    "    (\"genel_kodlama_ilgisi\",\"coding_for_kids\"),\n",
    "    (\"platformlar_araclar\",\"scratch\"),\n",
    "    (\"platformlar_araclar\",\"code_org\"),\n",
    "    (\"platformlar_araclar\",\"blockly\"),\n",
    "    (\"oyun_tabanli_ogrenme\",\"coding_games\")\n",
    "]\n",
    "\n",
    "# wide table: each selected term becomes a column\n",
    "dfs = []\n",
    "for cat, term in sel_terms:\n",
    "    s = (trends_long.query(\"category == @cat and term == @term\")\n",
    "         .set_index(\"date\")[\"interest\"].rename(f\"{cat}__{term}\"))\n",
    "    dfs.append(s)\n",
    "trends_sel_wide = pd.concat(dfs, axis=1)\n",
    "\n",
    "merged_for_next = scratch_wide.set_index(\"date\").join(trends_sel_wide, how=\"inner\").reset_index()\n",
    "merged_for_next.to_parquet(CLEAN / \"merged_step2.parquet\", index=False)\n",
    "merged_for_next.to_csv(CLEAN / \"merged_step2.csv\", index=False)\n",
    "\n",
    "print(\"Saved:\", CLEAN / \"merged_step2.parquet\")\n",
    "print(\"Columns:\", merged_for_next.columns.tolist())\n",
    "print(\"Date range:\", merged_for_next['date'].min().date(), \"→\", merged_for_next['date'].max().date())\n",
    "print(\"Sample rows:\\n\", merged_for_next.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca76e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2+ / Cell B1\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def cat_pivot(tr_df: pd.DataFrame, cat: str):\n",
    "    \"\"\"Convert the selected category to wide form (date x term).\"\"\"\n",
    "    df = (tr_df.query(\"category == @cat\")\n",
    "                .pivot(index=\"date\", columns=\"term\", values=\"interest\")\n",
    "                .sort_index())\n",
    "    df_ma = df.rolling(3, min_periods=1).mean()  # 3M MA\n",
    "    return df, df_ma\n",
    "\n",
    "def top_terms(df_ma: pd.DataFrame, k=5, mode=\"mean\"):\n",
    "    \"\"\"Top-K terms: 'mean' (entire period) or 'recent' (last 12 months avg.)\"\"\"\n",
    "    if mode == \"recent\":\n",
    "        ref = df_ma.tail(12).mean(numeric_only=True)\n",
    "    else:\n",
    "        ref = df_ma.mean(numeric_only=True)\n",
    "    return ref.sort_values(ascending=False).head(k).index.tolist()\n",
    "\n",
    "def minmax_0_100_cols(df: pd.DataFrame):\n",
    "    out = df.copy()\n",
    "    for c in out.columns:\n",
    "        s = out[c].astype(float)\n",
    "        lo, hi = s.min(), s.max()\n",
    "        out[c] = np.where(hi>lo, (s-lo)/(hi-lo)*100.0, np.nan)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19401d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2+ / Cell B2\n",
    "CATS = trends_long[\"category\"].unique().tolist()  # all categories\n",
    "K = 5  # number of terms to display per category\n",
    "\n",
    "for CAT in CATS:\n",
    "    df, df_ma = cat_pivot(trends_long, CAT)\n",
    "    terms = top_terms(df_ma, k=K, mode=\"mean\")\n",
    "    plt.figure(figsize=(11,4))\n",
    "    for t in terms:\n",
    "        plt.plot(df_ma.index, df_ma[t], label=t)\n",
    "    plt.title(f\"Google Trends — {CAT} | Top-{K} terms (3M MA)\")\n",
    "    plt.xlabel(\"Date\"); plt.ylabel(\"Interest (0–100)\")\n",
    "    plt.legend(loc=\"upper left\", ncol=2, fontsize=9)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f900033a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2+ / Cell B3\n",
    "for CAT in CATS:\n",
    "    df, df_ma = cat_pivot(trends_long, CAT)\n",
    "    terms = top_terms(df_ma, k=5, mode=\"recent\")  # most popular based on the last 12 months\n",
    "    df_norm = minmax_0_100_cols(df_ma[terms])\n",
    "\n",
    "    plt.figure(figsize=(11,4))\n",
    "    for t in terms:\n",
    "        plt.plot(df_norm.index, df_norm[t], label=t)\n",
    "    plt.title(f\"{CAT} | Top-5 (last 12 months popular) — normalized 0–100, 3M MA\")\n",
    "    plt.xlabel(\"Date\"); plt.ylabel(\"Normalized (0–100)\")\n",
    "    plt.legend(loc=\"upper left\", ncol=2, fontsize=9)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6932b6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2+ / Cell B4\n",
    "for CAT in CATS:\n",
    "    df, df_ma = cat_pivot(trends_long, CAT)\n",
    "    # only columns with sufficient data\n",
    "    df_ma = df_ma.loc[:, df_ma.notna().mean() > 0.7]\n",
    "    if df_ma.shape[1] < 2:\n",
    "        continue\n",
    "    corr = df_ma.corr()\n",
    "\n",
    "    plt.figure(figsize=(6,5))\n",
    "    im = plt.imshow(corr, vmin=-1, vmax=1)\n",
    "    plt.title(f\"{CAT} | Term-Term Correlation (3M MA)\")\n",
    "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "    ticks = np.arange(len(corr.columns))\n",
    "    plt.xticks(ticks, corr.columns, rotation=60, ha=\"right\", fontsize=8)\n",
    "    plt.yticks(ticks, corr.index, fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfaae51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2+ / Cell B5\n",
    "from numpy.polynomial.polynomial import polyfit\n",
    "\n",
    "def slope_last_n(series: pd.Series, n=12):\n",
    "    s = series.dropna().tail(n)\n",
    "    if len(s) < max(6, n//2):\n",
    "        return np.nan\n",
    "    # Normalize to 0–100 to make slope comparable\n",
    "    lo, hi = s.min(), s.max()\n",
    "    if hi <= lo:  # constant series\n",
    "        return 0.0\n",
    "    s = (s - lo) / (hi - lo) * 100.0\n",
    "    x = np.arange(len(s))\n",
    "    b1 = polyfit(x, s.values, 1)[1]  # slope\n",
    "    return b1\n",
    "\n",
    "for CAT in CATS:\n",
    "    df, df_ma = cat_pivot(trends_long, CAT)\n",
    "    slopes = {t: slope_last_n(df_ma[t], 12) for t in df_ma.columns}\n",
    "    top_up = pd.Series(slopes).sort_values(ascending=False).head(10).round(2)\n",
    "    print(f\"\\n=== {CAT} | Last 12 months 'momentum' (slope) — Top 10 increases ===\")\n",
    "    print(top_up.to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9540fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 / C1: Load, align, and pivot data to wide form\n",
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "CLEAN = Path(\"/kaggle/working/clean\")\n",
    "\n",
    "# The cleaned data we saved\n",
    "trends_long = pd.read_parquet(CLEAN / \"google_trends_long.parquet\")  # date, category, term, interest\n",
    "scratch_wide = pd.read_parquet(CLEAN / \"scratch_clean.parquet\")      # date, new_projects, new_users, new_comments\n",
    "\n",
    "# Fix dates to the start of the month\n",
    "def zfill_month(dt: pd.Series):\n",
    "    s = pd.to_datetime(dt, errors=\"coerce\")\n",
    "    return s.dt.to_period(\"M\").dt.to_timestamp(how=\"start\")\n",
    "\n",
    "trends_long[\"date\"] = zfill_month(trends_long[\"date\"])\n",
    "scratch_wide[\"date\"] = zfill_month(scratch_wide[\"date\"])\n",
    "\n",
    "# Trends → wide: each column 'category__term'\n",
    "tw = trends_long.pivot(index=\"date\", columns=[\"category\",\"term\"], values=\"interest\").sort_index()\n",
    "tw.columns = [f\"{a}__{b}\" for a,b in tw.columns]\n",
    "# Smooth slightly with a 3-month MA (reduces noise for correlation)\n",
    "tw_ma = tw.rolling(3, min_periods=1).mean()\n",
    "\n",
    "# Scratch → set index\n",
    "sw = scratch_wide.set_index(\"date\")[[\"new_projects\",\"new_users\",\"new_comments\"]].sort_index()\n",
    "sw_ma = sw.rolling(3, min_periods=1).mean()\n",
    "\n",
    "# Common range (around 2007-03 → 2024-06)\n",
    "start = max(tw_ma.index.min(), sw_ma.index.min())\n",
    "end   = min(tw_ma.index.max(), sw_ma.index.max())\n",
    "tw_ma = tw_ma.loc[start:end]\n",
    "sw_ma = sw_ma.loc[start:end]\n",
    "\n",
    "print(\"tw_ma shape:\", tw_ma.shape, \"| sw_ma shape:\", sw_ma.shape, \"| range:\", start.date(), \"→\", end.date())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6f68f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 / C2: Helpers (lagged correlation, z-score)\n",
    "\n",
    "def zscore(s: pd.Series):\n",
    "    s = s.astype(float)\n",
    "    return (s - s.mean()) / s.std(ddof=0)\n",
    "\n",
    "def corr_with_lags(tr_series: pd.Series, sc_series: pd.Series, lags=range(-12,13), method=\"pearson\"):\n",
    "    \"\"\"\n",
    "    tr_series: Trends series (x_t)\n",
    "    sc_series: Scratch metric (y_t)\n",
    "    Positive lag = Trends leads (correlation of x_t with y_{t+lag})\n",
    "                   (in practice we shift the scratch series forward by +lag)\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for k in lags:\n",
    "        sc_shift = sc_series.shift(k)\n",
    "        valid = tr_series.notna() & sc_shift.notna()\n",
    "        n = int(valid.sum())\n",
    "        if n >= 12:\n",
    "            x = tr_series[valid]\n",
    "            y = sc_shift[valid]\n",
    "            if method == \"pearson\":\n",
    "                r = x.corr(y)\n",
    "            else:\n",
    "                r = x.rank().corr(y.rank())\n",
    "            out.append({\"lag\": k, \"r\": r, \"n\": n})\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "print(\"Helpers ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6b868d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 / C3: Find the best correlations for all terms vs. each Scratch metric\n",
    "TERMS = tw_ma.columns.tolist()\n",
    "METRICS = [\"new_projects\",\"new_users\",\"new_comments\"]\n",
    "LAGS = range(-12, 13)\n",
    "\n",
    "rows = []\n",
    "for term in TERMS:\n",
    "    x = zscore(tw_ma[term])  # standardize\n",
    "    for m in METRICS:\n",
    "        y = zscore(sw_ma[m])\n",
    "        # Pearson\n",
    "        dfp = corr_with_lags(x, y, lags=LAGS, method=\"pearson\")\n",
    "        if not dfp.empty:\n",
    "            pbest = dfp.iloc[dfp[\"r\"].abs().argmax()]\n",
    "            rows.append({\"term\": term, \"metric\": m, \"method\":\"pearson\",\n",
    "                         \"best_lag\": int(pbest[\"lag\"]), \"r\": float(pbest[\"r\"]), \"n\": int(pbest[\"n\"])})\n",
    "        # Spearman\n",
    "        dfs = corr_with_lags(x, y, lags=LAGS, method=\"spearman\")\n",
    "        if not dfs.empty:\n",
    "            sbest = dfs.iloc[dfs[\"r\"].abs().argmax()]\n",
    "            rows.append({\"term\": term, \"metric\": m, \"method\":\"spearman\",\n",
    "                         \"best_lag\": int(sbest[\"lag\"]), \"r\": float(sbest[\"r\"]), \"n\": int(sbest[\"n\"])})\n",
    "\n",
    "corr_summary = pd.DataFrame(rows).sort_values(\"r\", key=lambda s: s.abs(), ascending=False)\n",
    "corr_summary.head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c013ed79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 / C4: Plot the top 6 matches\n",
    "import matplotlib.pyplot as plt\n",
    "def minmax_0_100(s: pd.Series):\n",
    "    s = s.astype(float)\n",
    "    lo, hi = s.min(), s.max()\n",
    "    return (s - lo) / (hi - lo) * 100.0 if hi>lo else s*0+50\n",
    "\n",
    "TOPK = 6\n",
    "top_pairs = corr_summary.head(TOPK)\n",
    "\n",
    "for _, row in top_pairs.iterrows():\n",
    "    term, metric, lag = row[\"term\"], row[\"metric\"], int(row[\"best_lag\"])\n",
    "    xt = tw_ma[term]\n",
    "    yt = sw_ma[metric].shift(lag)  # apply lag in the plot so the curves align\n",
    "\n",
    "    both = pd.concat([xt.rename(\"trend\"), yt.rename(metric)], axis=1).dropna()\n",
    "    both = both.apply(minmax_0_100)\n",
    "\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.plot(both.index, both[\"trend\"], label=f\"{term} (Trend)\")\n",
    "    plt.plot(both.index, both[metric], label=f\"{metric} (Lag={lag})\")\n",
    "    plt.title(f\"Match: {term} ↔ {metric} | Best lag={lag}, r={row['r']:.2f} ({row['method']})\")\n",
    "    plt.xlabel(\"Date\"); plt.ylabel(\"Normalized (0–100)\"); plt.legend(loc=\"upper left\")\n",
    "    plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18955aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 / C5: Granger (x causes y?) — Test the top 5 pairs\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from pmdarima.arima.utils import ndiffs\n",
    "import numpy as np\n",
    "\n",
    "def make_stationary(s: pd.Series):\n",
    "    s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    s = np.log1p(s)  # safe for zeros\n",
    "    d = max(ndiffs(s.dropna(), test='adf', max_d=2), 0)\n",
    "    s = s.diff(d) if d>0 else s\n",
    "    return s.dropna(), d\n",
    "\n",
    "def granger_x_to_y(x: pd.Series, y: pd.Series, maxlag=12):\n",
    "    df = pd.concat([y.rename(\"y\"), x.rename(\"x\")], axis=1).dropna()\n",
    "    if len(df) < maxlag*3:\n",
    "        return None  # data is short\n",
    "    y_s, dy = make_stationary(df[\"y\"])\n",
    "    x_s, dx = make_stationary(df[\"x\"])\n",
    "    df_s = pd.concat([y_s, x_s], axis=1).dropna()\n",
    "    if len(df_s) < maxlag*3:\n",
    "        return None\n",
    "    res = grangercausalitytests(df_s[[\"y\",\"x\"]], maxlag=maxlag, verbose=False)\n",
    "    out = []\n",
    "    for lag, r in res.items():\n",
    "        p = r[0][\"ssr_ftest\"][1]  # SSR F-test p-value\n",
    "        out.append({\"lag\": lag, \"p_value\": float(p)})\n",
    "    best = min(out, key=lambda d: d[\"p_value\"])\n",
    "    return {\"best_lag\": int(best[\"lag\"]), \"best_p\": float(best[\"p_value\"]), \"all\": out, \"d_y\": int(dy), \"d_x\": int(dx), \"n\": int(len(df_s))}\n",
    "\n",
    "# From the best 10 matches, take \"pearson\" results and select unique (term, metric)\n",
    "top_for_granger = (corr_summary.query(\"method=='pearson'\")\n",
    "                   .drop_duplicates(subset=[\"term\",\"metric\"])\n",
    "                   .head(10))\n",
    "\n",
    "grows = []\n",
    "for _, r in top_for_granger.iterrows():\n",
    "    term, metric = r[\"term\"], r[\"metric\"]\n",
    "    x = tw_ma[term]              # trend\n",
    "    y = sw_ma[metric]            # scratch metric\n",
    "    g = granger_x_to_y(x, y, maxlag=12)\n",
    "    if g:\n",
    "        g[\"term\"] = term; g[\"metric\"] = metric\n",
    "        grows.append(g)\n",
    "\n",
    "granger_summary = pd.DataFrame([{\n",
    "    \"term\": g[\"term\"], \"metric\": g[\"metric\"],\n",
    "    \"best_lag\": g[\"best_lag\"], \"best_p\": g[\"best_p\"],\n",
    "    \"d_x\": g[\"d_x\"], \"d_y\": g[\"d_y\"], \"n\": g[\"n\"]\n",
    "} for g in grows]).sort_values(\"best_p\")\n",
    "\n",
    "granger_summary.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4d4a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 / C6: Save outputs\n",
    "corr_summary.to_csv(CLEAN / \"corr_lag_summary.csv\", index=False)\n",
    "if 'granger_summary' in globals() and not granger_summary.empty:\n",
    "    granger_summary.to_csv(CLEAN / \"granger_summary.csv\", index=False)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\"-\", CLEAN / \"corr_lag_summary.csv\")\n",
    "print(\"-\", CLEAN / \"granger_summary.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8aee8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 / C7: Feedback text\n",
    "from textwrap import dedent\n",
    "\n",
    "granger_info = \"granger_summary.csv could not be created (insufficient data)\" \\\n",
    "               if ('granger_summary' not in globals() or granger_summary.empty) \\\n",
    "               else f\"Granger p-values computed for {len(granger_summary)} pairs; lowest p: {granger_summary['best_p'].min():.4f}\"\n",
    "\n",
    "best6 = corr_summary.head(6)[[\"term\",\"metric\",\"method\",\"best_lag\",\"r\",\"n\"]].copy()\n",
    "best6[\"r\"] = best6[\"r\"].round(3)\n",
    "\n",
    "msg = dedent(f\"\"\"\n",
    "=== STEP 3 COMPLETED ===\n",
    "\n",
    "[Summary]\n",
    "- Pearson/Spearman lag scan: {len(corr_summary)} relationships (all terms × 3 metrics × 2 methods).\n",
    "- Top 6 strongest relationships:\n",
    "{best6.to_string(index=False)}\n",
    "- Granger: {granger_info}\n",
    "\n",
    "[Files]\n",
    "- corr_lag_summary.csv\n",
    "- granger_summary.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2745b388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D1: Lag curves (Pearson & Spearman) — scratch vs new_users / new_comments\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "CLEAN = Path(\"/kaggle/working/clean\")\n",
    "trends_long = pd.read_parquet(CLEAN / \"google_trends_long.parquet\")\n",
    "scratch_wide = pd.read_parquet(CLEAN / \"scratch_clean.parquet\")\n",
    "\n",
    "def zfill_month(dt): \n",
    "    s = pd.to_datetime(dt, errors=\"coerce\"); \n",
    "    return s.dt.to_period(\"M\").dt.to_timestamp(how=\"start\")\n",
    "\n",
    "trends_long[\"date\"] = zfill_month(trends_long[\"date\"])\n",
    "scratch_wide[\"date\"] = zfill_month(scratch_wide[\"date\"])\n",
    "\n",
    "tw = (trends_long.pivot(index=\"date\", columns=[\"category\",\"term\"], values=\"interest\")\n",
    "                 .sort_index())\n",
    "tw.columns = [f\"{a}__{b}\" for a,b in tw.columns]\n",
    "tw = tw.rolling(3, min_periods=1).mean()\n",
    "\n",
    "sw = scratch_wide.set_index(\"date\")[[\"new_users\",\"new_comments\"]].rolling(3, min_periods=1).mean()\n",
    "\n",
    "x = tw[\"platformlar_araclar__scratch\"]\n",
    "pairs = {\"new_users\": sw[\"new_users\"], \"new_comments\": sw[\"new_comments\"]}\n",
    "\n",
    "def corr_with_lags(x, y, lags=range(-12,13), method=\"pearson\"):\n",
    "    rows=[]\n",
    "    for k in lags:\n",
    "        ys = y.shift(k)\n",
    "        v = x.notna() & ys.notna()\n",
    "        if v.sum()>=12:\n",
    "            r = (x[v].corr(ys[v]) if method==\"pearson\" else x[v].rank().corr(ys[v].rank()))\n",
    "            rows.append((k, r))\n",
    "    return pd.DataFrame(rows, columns=[\"lag\",\"r\"])\n",
    "\n",
    "for name,y in pairs.items():\n",
    "    plt.figure(figsize=(7,3.2))\n",
    "    dfp = corr_with_lags(x,y,method=\"pearson\")\n",
    "    dfs = corr_with_lags(x,y,method=\"spearman\")\n",
    "    plt.plot(dfp[\"lag\"], dfp[\"r\"], label=\"pearson\")\n",
    "    plt.plot(dfs[\"lag\"], dfs[\"r\"], label=\"spearman\", linestyle=\"--\")\n",
    "    plt.axvline(0,color=\"k\",alpha=.2); plt.axvline(12,color=\"k\",alpha=.1); plt.axvline(-12,color=\"k\",alpha=.1)\n",
    "    plt.title(f\"Lag curve: scratch (trend) ↔ {name}\")\n",
    "    plt.xlabel(\"Lag (months)  |  +: Trend leads\"); plt.ylabel(\"r\")\n",
    "    plt.legend(); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a7e851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D2: Lag=0 correlation after 12-month differencing (seasonality removal)\n",
    "import numpy as np\n",
    "\n",
    "def d12(s): \n",
    "    return np.log1p(s).diff(12)  # log + 12-month difference\n",
    "\n",
    "x_d = d12(tw[\"platformlar_araclar__scratch\"])\n",
    "users_d = d12(sw[\"new_users\"])\n",
    "comments_d = d12(sw[\"new_comments\"])\n",
    "\n",
    "for name, y in {\"new_users\": users_d, \"new_comments\": comments_d}.items():\n",
    "    v = x_d.notna() & y.notna()\n",
    "    if v.sum()>=24:\n",
    "        print(f\"{name} | Pearson r@lag0 (d12):\", round(x_d[v].corr(y[v]), 3))\n",
    "    else:\n",
    "        print(f\"{name} | data is short (d12)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4762baa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 / C5-Seasonal: Re-test Granger with seasonal differencing (1..6 months)\n",
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "from statsmodels.tsa.stattools import grangercausalitytests, adfuller\n",
    "\n",
    "CLEAN = Path(\"/kaggle/working/clean\")\n",
    "\n",
    "# --- Load data (raw, no MA) ---\n",
    "tr = pd.read_parquet(CLEAN / \"google_trends_long.parquet\")   # date, category, term, interest\n",
    "sc = pd.read_parquet(CLEAN / \"scratch_clean.parquet\")        # date, new_projects, new_users, new_comments\n",
    "\n",
    "def zfill_month(dt):\n",
    "    s = pd.to_datetime(dt, errors=\"coerce\")\n",
    "    return s.dt.to_period(\"M\").dt.to_timestamp(how=\"start\")\n",
    "\n",
    "tr[\"date\"] = zfill_month(tr[\"date\"])\n",
    "sc[\"date\"] = zfill_month(sc[\"date\"])\n",
    "\n",
    "# Trends -> wide (category__term columns)\n",
    "tw = tr.pivot(index=\"date\", columns=[\"category\",\"term\"], values=\"interest\").sort_index()\n",
    "tw.columns = [f\"{a}__{b}\" for a,b in tw.columns]\n",
    "\n",
    "# Scratch -> wide\n",
    "sw = sc.set_index(\"date\")[[\"new_projects\",\"new_users\",\"new_comments\"]].sort_index()\n",
    "\n",
    "# Common range\n",
    "start = max(tw.index.min(), sw.index.min())\n",
    "end   = min(tw.index.max(), sw.index.max())\n",
    "tw = tw.loc[start:end]\n",
    "sw = sw.loc[start:end]\n",
    "\n",
    "# --- Helpers ---\n",
    "def seasonal_stationary(s: pd.Series):\n",
    "    \"\"\"log1p -> diff(12) [+ diff(1) if ADF p>0.05]\"\"\"\n",
    "    s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    s = np.log1p(s).diff(12).dropna()\n",
    "    used = \"log1p -> diff(12)\"\n",
    "    try:\n",
    "        p = adfuller(s.dropna(), autolag=\"AIC\")[1]\n",
    "    except Exception:\n",
    "        p = np.nan\n",
    "    if np.isnan(p) or p > 0.05:\n",
    "        s = s.diff().dropna()\n",
    "        used += \" -> diff(1)\"\n",
    "    return s, used\n",
    "\n",
    "def granger_test_pair(x: pd.Series, y: pd.Series, maxlag=6):\n",
    "    df = pd.concat([y.rename(\"y\"), x.rename(\"x\")], axis=1).dropna()\n",
    "    if len(df) < maxlag * 4:   # do not trust very short series\n",
    "        return None\n",
    "    res = grangercausalitytests(df[[\"y\",\"x\"]], maxlag=maxlag, verbose=False)\n",
    "    rows = []\n",
    "    for lag, r in res.items():\n",
    "        p = r[0][\"ssr_ftest\"][1]  # F-test p-value\n",
    "        rows.append((lag, p))\n",
    "    best = min(rows, key=lambda t: t[1])\n",
    "    return {\"best_lag\": int(best[0]), \"best_p\": float(best[1])}\n",
    "\n",
    "# Candidate pairs: if corr_lag_summary.csv exists, take the top 10 strongest pairs from there; otherwise sensible defaults\n",
    "try:\n",
    "    cs = pd.read_csv(CLEAN / \"corr_lag_summary.csv\")\n",
    "    cs = cs[cs[\"method\"] == \"pearson\"].drop_duplicates(subset=[\"term\",\"metric\"])\n",
    "    pairs = (cs.sort_values(\"r\", key=lambda s: s.abs(), ascending=False)\n",
    "               .head(10)[[\"term\",\"metric\"]].itertuples(index=False, name=None))\n",
    "    pairs = list(pairs)\n",
    "except FileNotFoundError:\n",
    "    pairs = [\n",
    "        (\"platformlar_araclar__scratch\",\"new_users\"),\n",
    "        (\"platformlar_araclar__scratch\",\"new_projects\"),\n",
    "        (\"platformlar_araclar__scratch\",\"new_comments\"),\n",
    "        (\"egitsel_baglam__stem_for_kids\",\"new_users\"),\n",
    "        (\"egitsel_baglam__stem_for_kids\",\"new_projects\")\n",
    "    ]\n",
    "\n",
    "# --- Test ---\n",
    "out = []\n",
    "for term, metric in pairs:\n",
    "    if term not in tw.columns or metric not in sw.columns:\n",
    "        continue\n",
    "    x_raw = tw[term]\n",
    "    y_raw = sw[metric]\n",
    "    x_s, x_tr = seasonal_stationary(x_raw)\n",
    "    y_s, y_tr = seasonal_stationary(y_raw)\n",
    "\n",
    "    df = pd.concat([y_s.rename(\"y\"), x_s.rename(\"x\")], axis=1).dropna()\n",
    "    if len(df) < 36:\n",
    "        out.append({\"term\": term, \"metric\": metric, \"n\": len(df), \"best_lag\": None, \"best_p\": None,\n",
    "                    \"sig(p<0.05)\": False, \"x_transform\": x_tr, \"y_transform\": y_tr, \"note\": \"short series\"})\n",
    "        continue\n",
    "\n",
    "    g = granger_test_pair(df[\"x\"], df[\"y\"], maxlag=6)\n",
    "    if g:\n",
    "        out.append({\"term\": term, \"metric\": metric, \"n\": len(df),\n",
    "                    \"best_lag\": g[\"best_lag\"], \"best_p\": g[\"best_p\"],\n",
    "                    \"sig(p<0.05)\": g[\"best_p\"] < 0.05,\n",
    "                    \"x_transform\": x_tr, \"y_transform\": y_tr})\n",
    "    else:\n",
    "        out.append({\"term\": term, \"metric\": metric, \"n\": len(df), \"best_lag\": None, \"best_p\": None,\n",
    "                    \"sig(p<0.05)\": False, \"x_transform\": x_tr, \"y_transform\": y_tr})\n",
    "\n",
    "granger_seasonal = pd.DataFrame(out).sort_values(\"best_p\", na_position=\"last\")\n",
    "print(\"=== Granger (with seasonal differencing) — summary ===\")\n",
    "if not granger_seasonal.empty:\n",
    "    print(granger_seasonal.head(10).to_string(index=False))\n",
    "    outpath = CLEAN / \"granger_seasonal_summary.csv\"\n",
    "    granger_seasonal.to_csv(outpath, index=False)\n",
    "    print(\"\\nSaved:\", outpath)\n",
    "else:\n",
    "    print(\"No suitable result could be produced (data may be insufficient).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5c6a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 / E1: Data and split\n",
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "CLEAN = Path(\"/kaggle/working/clean\")\n",
    "trends_long = pd.read_parquet(CLEAN / \"google_trends_long.parquet\")   # date, category, term, interest\n",
    "scratch_wide = pd.read_parquet(CLEAN / \"scratch_clean.parquet\")       # date, new_projects, new_users, new_comments\n",
    "corr_sum = pd.read_csv(CLEAN / \"corr_lag_summary.csv\")                # from Step 3\n",
    "granger_seas = pd.read_csv(CLEAN / \"granger_seasonal_summary.csv\")    # if available\n",
    "\n",
    "def mstart(s): \n",
    "    s = pd.to_datetime(s, errors=\"coerce\")\n",
    "    return s.dt.to_period(\"M\").dt.to_timestamp(how=\"start\")\n",
    "\n",
    "trends_long[\"date\"] = mstart(trends_long[\"date\"])\n",
    "scratch_wide[\"date\"] = mstart(scratch_wide[\"date\"])\n",
    "\n",
    "# Target METRIC: you can switch to 'new_projects' or 'new_comments' if you like.\n",
    "TARGET = \"new_users\"\n",
    "\n",
    "y = scratch_wide.set_index(\"date\")[TARGET].asfreq(\"MS\")\n",
    "# Train/Test: last 24 months as test\n",
    "TEST_H = 24\n",
    "y_train, y_test = y.iloc[:-TEST_H], y.iloc[-TEST_H:]\n",
    "\n",
    "print(\"Target:\", TARGET)\n",
    "print(\"Train range:\", y_train.index.min().date(), \"→\", y_train.index.max().date(), \"| n=\", len(y_train))\n",
    "print(\"Test  range:\", y_test.index.min().date(), \"→\", y_test.index.max().date(), \"| n=\", len(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab18e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 / E2: Exog candidates\n",
    "# 1) From 'pearson' results (lag 0..6), the top 3 terms strongest with TARGET\n",
    "c = (corr_sum.query(\"method=='pearson' and metric==@TARGET\")\n",
    "             .query(\"best_lag>=0 and best_lag<=6\")\n",
    "             .sort_values(\"r\", key=lambda s: s.abs(), ascending=False))\n",
    "top_terms = c.drop_duplicates(\"term\").head(3)[[\"term\",\"best_lag\"]].values.tolist()\n",
    "\n",
    "# 2) Also include terms that were significant in Seasonal Granger (p<0.05)\n",
    "if not granger_seas.empty:\n",
    "    gsel = granger_seas.query(\"metric==@TARGET and `sig(p<0.05)`==True\")[[\"term\",\"best_lag\"]].values.tolist()\n",
    "else:\n",
    "    gsel = []\n",
    "\n",
    "# 3) Safe fallback: scratch term lag0\n",
    "fallback = [(\"platformlar_araclar__scratch\", 0)]\n",
    "candidates = (top_terms + gsel + fallback)\n",
    "\n",
    "# Deduplicate\n",
    "seen = set(); exog_terms = []\n",
    "for t, lag in candidates:\n",
    "    if t not in seen and t in trends_long.assign(col=trends_long[\"category\"]+\"__\"+trends_long[\"term\"])[\"col\"].unique():\n",
    "        exog_terms.append((t, int(lag)))\n",
    "        seen.add(t)\n",
    "\n",
    "print(\"Exog candidates (term, lag):\", exog_terms[:6])\n",
    "\n",
    "# Build the exog matrix\n",
    "tw = trends_long.pivot(index=\"date\", columns=[\"category\",\"term\"], values=\"interest\")\n",
    "tw.columns = [f\"{a}__{b}\" for a,b in tw.columns]\n",
    "tw = tw.asfreq(\"MS\")\n",
    "\n",
    "def build_exog(exog_specs, ma=3):\n",
    "    X = pd.DataFrame(index=tw.index)\n",
    "    for term, lag in exog_specs:\n",
    "        s = tw[term].rolling(ma, min_periods=1).mean().shift(lag)  # small MA + specified lag\n",
    "        X[f\"{term}__lag{lag}\"] = s\n",
    "    return X\n",
    "\n",
    "X_all = build_exog(exog_terms)\n",
    "X_train, X_test = X_all.loc[y_train.index], X_all.loc[y_test.index]\n",
    "print(\"Exog shape:\", X_all.shape, \"| Train/Test:\", X_train.shape, X_test.shape)\n",
    "print(\"First columns:\", X_all.columns[:6].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5d123f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 / E3: Baseline SARIMA specification (auto_arima with y only)\n",
    "from pmdarima import auto_arima\n",
    "\n",
    "# silence warnings\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "sarima_model = auto_arima(y_train, seasonal=True, m=12,\n",
    "                          information_criterion=\"aic\",\n",
    "                          stepwise=True, trace=False,\n",
    "                          suppress_warnings=True,\n",
    "                          error_action=\"ignore\")\n",
    "order = sarima_model.order\n",
    "seasonal_order = sarima_model.seasonal_order\n",
    "\n",
    "print(\"Selected SARIMA order:\", order, seasonal_order)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bba69d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 / E4: Baseline SARIMA fit & forecast\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import numpy as np\n",
    "\n",
    "base = SARIMAX(y_train, order=order, seasonal_order=seasonal_order,\n",
    "               enforce_stationarity=False, enforce_invertibility=False).fit(disp=False)\n",
    "base_fc = base.get_forecast(steps=TEST_H).predicted_mean\n",
    "base_fc.index = y_test.index\n",
    "\n",
    "def metrics(y_true, y_pred):\n",
    "    e = (y_pred - y_true).astype(float)\n",
    "    mae = e.abs().mean()\n",
    "    rmse = np.sqrt((e**2).mean())\n",
    "    mape = (e.abs() / y_true.replace(0, np.nan)).mean() * 100\n",
    "    return {\"MAE\": float(mae), \"RMSE\": float(rmse), \"MAPE%\": float(mape)}\n",
    "\n",
    "base_metrics = metrics(y_test, base_fc)\n",
    "print(\"Baseline (SARIMA) metrics:\", base_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3a3ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 / E5: SARIMAX variants (individually and all together)\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "results = []\n",
    "# Each exog individually\n",
    "for i in range(min(3, len(X_train.columns))):\n",
    "    name = X_train.columns[i]\n",
    "    try:\n",
    "        model = SARIMAX(y_train, exog=X_train[[name]], order=order, seasonal_order=seasonal_order,\n",
    "                        enforce_stationarity=False, enforce_invertibility=False).fit(disp=False)\n",
    "        fc = model.get_forecast(steps=TEST_H, exog=X_test[[name]]).predicted_mean\n",
    "        r = metrics(y_test, fc)\n",
    "        r.update({\"model\":\"SARIMAX\", \"exog\":[name]})\n",
    "        results.append(r)\n",
    "    except Exception as e:\n",
    "        results.append({\"model\":\"SARIMAX\",\"exog\":[name],\"error\":str(e)})\n",
    "\n",
    "# All exogs together (if many columns, take the first 3)\n",
    "exog_cols = X_train.columns[:min(3, len(X_train.columns))]\n",
    "if len(exog_cols) >= 2:\n",
    "    try:\n",
    "        model = SARIMAX(y_train, exog=X_train[exog_cols], order=order, seasonal_order=seasonal_order,\n",
    "                        enforce_stationarity=False, enforce_invertibility=False).fit(disp=False)\n",
    "        fc = model.get_forecast(steps=TEST_H, exog=X_test[exog_cols]).predicted_mean\n",
    "        r = metrics(y_test, fc)\n",
    "        r.update({\"model\":\"SARIMAX\",\"exog\":list(exog_cols)})\n",
    "        results.append(r)\n",
    "    except Exception as e:\n",
    "        results.append({\"model\":\"SARIMAX\",\"exog\":list(exog_cols),\"error\":str(e)})\n",
    "\n",
    "res = pd.DataFrame(results).sort_values(\"RMSE\", na_position=\"last\")\n",
    "print(\"SARIMAX results (best first):\\n\", res.head(6).to_string(index=False))\n",
    "best = res.iloc[0] if not res.empty else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6579bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 / E6: Plot and save outputs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "out_tbl = pd.DataFrame({\"actual\": y_test})\n",
    "out_tbl[\"baseline\"] = base_fc\n",
    "\n",
    "if best is not None and \"error\" not in best:\n",
    "    best_ex = best[\"exog\"]\n",
    "    ex_cols = best_ex if isinstance(best_ex, list) else [best_ex]\n",
    "    best_model = SARIMAX(y_train, exog=X_train[ex_cols], order=order, seasonal_order=seasonal_order,\n",
    "                         enforce_stationarity=False, enforce_invertibility=False).fit(disp=False)\n",
    "    best_fc = best_model.get_forecast(steps=TEST_H, exog=X_test[ex_cols]).predicted_mean\n",
    "    best_fc.index = y_test.index\n",
    "    out_tbl[\"exog_best\"] = best_fc\n",
    "else:\n",
    "    out_tbl[\"exog_best\"] = np.nan\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(y.index, y, label=\"actual (train+test)\")\n",
    "plt.plot(out_tbl.index, out_tbl[\"baseline\"], label=\"baseline (SARIMA)\")\n",
    "if out_tbl[\"exog_best\"].notna().any():\n",
    "    plt.plot(out_tbl.index, out_tbl[\"exog_best\"], label=\"exog_best (SARIMAX)\")\n",
    "plt.title(f\"Forecast — {TARGET} (last {TEST_H} months test)\")\n",
    "plt.xlabel(\"Date\"); plt.ylabel(TARGET); plt.legend(loc=\"upper left\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Save\n",
    "out_tbl.to_csv(CLEAN / f\"forecast_{TARGET}.csv\")\n",
    "print(\"Saved:\", CLEAN / f\"forecast_{TARGET}.csv\")\n",
    "print(\"Baseline:\", base_metrics)\n",
    "if out_tbl[\"exog_best\"].notna().any():\n",
    "    print(\"Exog_best:\", metrics(y_test, out_tbl[\"exog_best\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e9530b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 / E8: Baseline vs SARIMAX comparison summary\n",
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from pmdarima import auto_arima\n",
    "\n",
    "CLEAN = Path(\"/kaggle/working/clean\")\n",
    "TARGET = \"new_users\"  # If you selected a different target in E1, it should be the same here as well\n",
    "\n",
    "def mstart(s): \n",
    "    s = pd.to_datetime(s, errors=\"coerce\")\n",
    "    return s.dt.to_period(\"M\").dt.to_timestamp(how=\"start\")\n",
    "\n",
    "def metrics(y_true, y_pred):\n",
    "    e = (y_pred - y_true).astype(float)\n",
    "    mae = e.abs().mean()\n",
    "    rmse = np.sqrt((e**2).mean())\n",
    "    mape = (e.abs() / y_true.replace(0, np.nan)).mean() * 100\n",
    "    return {\"MAE\": float(mae), \"RMSE\": float(rmse), \"MAPE%\": float(mape)}\n",
    "\n",
    "# --- load data ---\n",
    "tr = pd.read_parquet(CLEAN / \"google_trends_long.parquet\")   # date, category, term, interest\n",
    "sc = pd.read_parquet(CLEAN / \"scratch_clean.parquet\")        # date, new_projects/new_users/new_comments\n",
    "tr[\"date\"] = mstart(tr[\"date\"]); sc[\"date\"] = mstart(sc[\"date\"])\n",
    "\n",
    "y = sc.set_index(\"date\")[TARGET].asfreq(\"MS\")\n",
    "TEST_H = 24\n",
    "y_train, y_test = y.iloc[:-TEST_H], y.iloc[-TEST_H:]\n",
    "\n",
    "# read from baseline forecast file (E6)\n",
    "fc_path = CLEAN / f\"forecast_{TARGET}.csv\"\n",
    "out_tbl = pd.read_csv(fc_path, index_col=0, parse_dates=True)\n",
    "base_metrics = metrics(y_test, out_tbl[\"baseline\"])\n",
    "has_sarimax = out_tbl[\"exog_best\"].notna().any()\n",
    "sarimax_metrics = metrics(y_test, out_tbl[\"exog_best\"]) if has_sarimax else None\n",
    "\n",
    "# --- rebuild exog candidates (summary of E2) ---\n",
    "corr_sum = pd.read_csv(CLEAN / \"corr_lag_summary.csv\")\n",
    "try:\n",
    "    gseas = pd.read_csv(CLEAN / \"granger_seasonal_summary.csv\")\n",
    "except FileNotFoundError:\n",
    "    gseas = pd.DataFrame(columns=[\"term\",\"metric\",\"best_lag\",\"sig(p<0.05)\"])\n",
    "\n",
    "tw = tr.pivot(index=\"date\", columns=[\"category\",\"term\"], values=\"interest\").asfreq(\"MS\").sort_index()\n",
    "tw.columns = [f\"{a}__{b}\" for a,b in tw.columns]\n",
    "\n",
    "c = (corr_sum.query(\"method=='pearson' and metric==@TARGET\")\n",
    "             .query(\"best_lag>=0 and best_lag<=6\")\n",
    "             .sort_values(\"r\", key=lambda s: s.abs(), ascending=False))\n",
    "top_terms = c.drop_duplicates(\"term\").head(3)[[\"term\",\"best_lag\"]].values.tolist()\n",
    "gsel = gseas.query(\"metric==@TARGET and `sig(p<0.05)`==True\")[[\"term\",\"best_lag\"]].values.tolist() if not gseas.empty else []\n",
    "candidates = top_terms + gsel + [(\"platforms_tools__scratch\", 0)]\n",
    "\n",
    "seen=set(); exog_specs=[]\n",
    "for t,lag in candidates:\n",
    "    if t in tw.columns and t not in seen:\n",
    "        exog_specs.append((t,int(lag))); seen.add(t)\n",
    "\n",
    "def build_exog(exog_specs, ma=3):\n",
    "    X = pd.DataFrame(index=tw.index)\n",
    "    for term, lag in exog_specs:\n",
    "        X[f\"{term}__lag{lag}\"] = tw[term].rolling(ma, min_periods=1).mean().shift(lag)\n",
    "    return X\n",
    "\n",
    "X_all = build_exog(exog_specs)\n",
    "X_train, X_test = X_all.loc[y_train.index], X_all.loc[y_test.index]\n",
    "\n",
    "# If E5 results are not in memory, retry briefly\n",
    "try:\n",
    "    res\n",
    "    best\n",
    "except NameError:\n",
    "    # Find SARIMA specification again\n",
    "    sarima_model = auto_arima(y_train, seasonal=True, m=12, information_criterion=\"aic\",\n",
    "                              stepwise=True, trace=False, suppress_warnings=True, error_action=\"ignore\")\n",
    "    order, seasonal_order = sarima_model.order, sarima_model.seasonal_order\n",
    "    # baseline\n",
    "    base_fc = SARIMAX(y_train, order=order, seasonal_order=seasonal_order,\n",
    "                      enforce_stationarity=False, enforce_invertibility=False).fit(disp=False)\\\n",
    "                      .get_forecast(steps=TEST_H).predicted_mean\n",
    "    # single exog and all together\n",
    "    results=[]\n",
    "    cols = X_train.columns[:min(3, len(X_train.columns))]\n",
    "    for name in cols:\n",
    "        try:\n",
    "            m = SARIMAX(y_train, exog=X_train[[name]], order=order, seasonal_order=seasonal_order,\n",
    "                        enforce_stationarity=False, enforce_invertibility=False).fit(disp=False)\n",
    "            fc = m.get_forecast(steps=TEST_H, exog=X_test[[name]]).predicted_mean\n",
    "            r = metrics(y_test, fc); r.update({\"model\":\"SARIMAX\",\"exog\":[name]}); results.append(r)\n",
    "        except Exception as e:\n",
    "            results.append({\"model\":\"SARIMAX\",\"exog\":[name],\"error\":str(e)})\n",
    "    if len(cols) >= 2:\n",
    "        try:\n",
    "            m = SARIMAX(y_train, exog=X_train[cols], order=order, seasonal_order=seasonal_order,\n",
    "                        enforce_stationarity=False, enforce_invertibility=False).fit(disp=False)\n",
    "            fc = m.get_forecast(steps=TEST_H, exog=X_test[cols]).predicted_mean\n",
    "            r = metrics(y_test, fc); r.update({\"model\":\"SARIMAX\",\"exog\":list(cols)}); results.append(r)\n",
    "        except Exception as e:\n",
    "            results.append({\"model\":\"SARIMAX\",\"exog\":list(cols),\"error\":str(e)})\n",
    "    res = pd.DataFrame(results).sort_values(\"RMSE\", na_position=\"last\")\n",
    "    best = res.iloc[0] if not res.empty else None\n",
    "\n",
    "# print summary\n",
    "def fmt(m): \n",
    "    return f\"MAE={m['MAE']:.0f} | RMSE={m['RMSE']:.0f} | MAPE={m['MAPE%']:.2f}%\"\n",
    "\n",
    "print(\"\\n=== EVALUATION SUMMARY ===\")\n",
    "print(\"Target:\", TARGET)\n",
    "print(\"Baseline:\", fmt(base_metrics))\n",
    "if has_sarimax:\n",
    "    print(\"From file — SARIMAX(exog_best):\", fmt(sarimax_metrics))\n",
    "else:\n",
    "    print(\"From file — SARIMAX(exog_best): not available (E6 exog_best empty)\")\n",
    "\n",
    "if best is not None and \"error\" not in best:\n",
    "    print(\"\\nRe-run check — Best SARIMAX exog:\", best[\"exog\"])\n",
    "    print(\"Best SARIMAX metrics:\", f\"MAE={best['MAE']:.0f} | RMSE={best['RMSE']:.0f} | MAPE={best['MAPE%']:.2f}%\")\n",
    "    # improvement\n",
    "    def improv(b, s): return {k: (b[k]-s[k])/b[k]*100 for k in [\"MAE\",\"RMSE\",\"MAPE%\"]}\n",
    "    imp = improv(base_metrics, {\"MAE\":best[\"MAE\"],\"RMSE\":best[\"RMSE\"],\"MAPE%\":best[\"MAPE%\"]})\n",
    "    print(\"Improvement (−% better):\", {k: f\"{v:.1f}%\" for k,v in imp.items()})\n",
    "else:\n",
    "    print(\"\\nRe-run check — Best SARIMAX could not be calculated or an error occurred.\")\n",
    "\n",
    "print(\"\\nFirst 6 variants (if any):\")\n",
    "try:\n",
    "    print(res.head(6).to_string(index=False))\n",
    "except:\n",
    "    print(\"res table not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d16c73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1: sNaive baseline (y[t-12]) ile kıyas\n",
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "CLEAN = Path(\"/kaggle/working/clean\")\n",
    "TARGET = \"new_users\"  # başka hedef deniyorsan bunu değiştir\n",
    "\n",
    "y = pd.read_parquet(CLEAN / \"scratch_clean.parquet\").set_index(\"date\")[TARGET].asfreq(\"MS\")\n",
    "TEST_H = 24\n",
    "y_train, y_test = y.iloc[:-TEST_H], y.iloc[-TEST_H:]\n",
    "\n",
    "# sNaive: test dönemi için geçen yılın aynı ayı\n",
    "snaive_fc = y.shift(12).iloc[-TEST_H:]\n",
    "assert len(snaive_fc) == len(y_test)\n",
    "\n",
    "# E6 çıktısından baseline ve varsa en iyi SARIMAX\n",
    "out_tbl = pd.read_csv(CLEAN / f\"forecast_{TARGET}.csv\", index_col=0, parse_dates=True)\n",
    "base_fc = out_tbl[\"baseline\"]\n",
    "exog_fc = out_tbl[\"exog_best\"] if out_tbl[\"exog_best\"].notna().any() else None\n",
    "\n",
    "def metrics(y_true, y_pred):\n",
    "    e = (y_pred - y_true).astype(float)\n",
    "    return {\n",
    "        \"MAE\": float(e.abs().mean()),\n",
    "        \"RMSE\": float(np.sqrt((e**2).mean())),\n",
    "        \"MAPE%\": float((e.abs()/y_true.replace(0,np.nan)).mean()*100)\n",
    "    }\n",
    "\n",
    "print(\"=== F1: sNaive vs SARIMA vs SARIMAX ===\")\n",
    "print(\"sNaive:\", metrics(y_test, snaive_fc))\n",
    "print(\"SARIMA:\", metrics(y_test, base_fc))\n",
    "if exog_fc is not None:\n",
    "    print(\"SARIMAX(exog_best):\", metrics(y_test, exog_fc))\n",
    "else:\n",
    "    print(\"SARIMAX(exog_best): yok\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77939d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F2: En iyi SARIMAX için artık tanı (FIX: to_timestamp -> how=\"start\")\n",
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from pmdarima import auto_arima\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "CLEAN = Path(\"/kaggle/working/clean\")\n",
    "TARGET = \"new_users\"\n",
    "\n",
    "def mstart(s):\n",
    "    s = pd.to_datetime(s, errors=\"coerce\")\n",
    "    return s.dt.to_period(\"M\").dt.to_timestamp(how=\"start\")\n",
    "\n",
    "# veriler ve exog'lar\n",
    "tr = pd.read_parquet(CLEAN / \"google_trends_long.parquet\")\n",
    "sc = pd.read_parquet(CLEAN / \"scratch_clean.parquet\")\n",
    "tr[\"date\"] = mstart(tr[\"date\"])\n",
    "sc[\"date\"] = mstart(sc[\"date\"])\n",
    "y = sc.set_index(\"date\")[TARGET].asfreq(\"MS\")\n",
    "\n",
    "tw = tr.pivot(index=\"date\", columns=[\"category\",\"term\"], values=\"interest\").asfreq(\"MS\").sort_index()\n",
    "tw.columns = [f\"{a}__{b}\" for a,b in tw.columns]\n",
    "\n",
    "# E8'deki seçim mantığı (ilk 3 güçlü aday, lag 0..6)\n",
    "corr = pd.read_csv(CLEAN / \"corr_lag_summary.csv\")\n",
    "cand = (corr.query(\"method=='pearson' and metric==@TARGET\")\n",
    "             .query(\"best_lag>=0 and best_lag<=6\")\n",
    "             .sort_values(\"r\", key=lambda s: s.abs(), ascending=False)\n",
    "             .drop_duplicates(\"term\").head(3))[[\"term\",\"best_lag\"]].values.tolist()\n",
    "\n",
    "def build_exog(specs, ma=3):\n",
    "    X = pd.DataFrame(index=tw.index)\n",
    "    for t,lag in specs:\n",
    "        X[f\"{t}__lag{lag}\"] = tw[t].rolling(ma, min_periods=1).mean().shift(int(lag))\n",
    "    return X\n",
    "\n",
    "X = build_exog(cand)\n",
    "TEST_H=24\n",
    "y_tr, y_te = y.iloc[:-TEST_H], y.iloc[-TEST_H:]\n",
    "X_tr, X_te = X.loc[y_tr.index], X.loc[y_te.index]\n",
    "\n",
    "# SARIMA düzeni\n",
    "a = auto_arima(y_tr, seasonal=True, m=12, information_criterion=\"aic\",\n",
    "               stepwise=True, suppress_warnings=True)\n",
    "order, seasonal_order = a.order, a.seasonal_order\n",
    "\n",
    "m = SARIMAX(y_tr, exog=X_tr, order=order, seasonal_order=seasonal_order,\n",
    "            enforce_stationarity=False, enforce_invertibility=False).fit(disp=False)\n",
    "\n",
    "# standardized residuals\n",
    "resid = pd.Series(m.filter_results.standardized_forecasts_error[0], index=y_tr.index).dropna()\n",
    "\n",
    "print(\"=== F2: Ljung–Box p-değerleri ===\")\n",
    "print(acorr_ljungbox(resid, lags=[12,24], return_df=True))\n",
    "\n",
    "plt.figure(figsize=(7,3))\n",
    "plot_acf(resid, lags=24)\n",
    "plt.title(\"Artık ACF (SARIMAX)\"); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170bd846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F3: Rolling-origin backtest — SARIMA vs SARIMAX (FIX: mstart)\n",
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "from pmdarima import auto_arima\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "CLEAN = Path(\"/kaggle/working/clean\")\n",
    "TARGET = \"new_users\"\n",
    "\n",
    "def mstart(s):\n",
    "    s = pd.to_datetime(s, errors=\"coerce\")\n",
    "    return s.dt.to_period(\"M\").dt.to_timestamp(how=\"start\")\n",
    "\n",
    "tr = pd.read_parquet(CLEAN / \"google_trends_long.parquet\")\n",
    "sc = pd.read_parquet(CLEAN / \"scratch_clean.parquet\")\n",
    "tr[\"date\"] = mstart(tr[\"date\"])\n",
    "sc[\"date\"] = mstart(sc[\"date\"])\n",
    "y = sc.set_index(\"date\")[TARGET].asfreq(\"MS\")\n",
    "tw = tr.pivot(index=\"date\", columns=[\"category\",\"term\"], values=\"interest\").asfreq(\"MS\").sort_index()\n",
    "tw.columns = [f\"{a}__{b}\" for a,b in tw.columns]\n",
    "\n",
    "corr = pd.read_csv(CLEAN / \"corr_lag_summary.csv\")\n",
    "cand = (corr.query(\"method=='pearson' and metric==@TARGET\")\n",
    "             .query(\"best_lag>=0 and best_lag<=6\")\n",
    "             .sort_values(\"r\", key=lambda s: s.abs(), ascending=False)\n",
    "             .drop_duplicates(\"term\").head(3))[[\"term\",\"best_lag\"]].values.tolist()\n",
    "\n",
    "def build_exog(specs, ma=3):\n",
    "    X = pd.DataFrame(index=tw.index)\n",
    "    for t,lag in specs:\n",
    "        X[f\"{t}__lag{lag}\"] = tw[t].rolling(ma, min_periods=1).mean().shift(int(lag))\n",
    "    return X\n",
    "\n",
    "X = build_exog(cand)\n",
    "\n",
    "def rmse(a,b): \n",
    "    e=(a-b).astype(float); \n",
    "    return float(np.sqrt((e**2).mean()))\n",
    "\n",
    "folds=5; horizon=12\n",
    "r_sarima=[]; r_sarimax=[]\n",
    "for k in range(folds,0,-1):\n",
    "    split_end = -k*horizon\n",
    "    y_tr = y.iloc[:split_end] if split_end!=0 else y\n",
    "    y_te = y.iloc[split_end: split_end + horizon]\n",
    "    if len(y_te)<horizon or len(y_tr)<60: \n",
    "        continue\n",
    "    X_tr, X_te = X.loc[y_tr.index], X.loc[y_te.index]\n",
    "\n",
    "    a = auto_arima(y_tr, seasonal=True, m=12, information_criterion=\"aic\",\n",
    "                   stepwise=True, suppress_warnings=True)\n",
    "    order, so = a.order, a.seasonal_order\n",
    "\n",
    "    sarima = SARIMAX(y_tr, order=order, seasonal_order=so,\n",
    "                     enforce_stationarity=False, enforce_invertibility=False).fit(disp=False)\n",
    "    fc0 = sarima.get_forecast(steps=horizon).predicted_mean\n",
    "\n",
    "    sarimax = SARIMAX(y_tr, exog=X_tr, order=order, seasonal_order=so,\n",
    "                      enforce_stationarity=False, enforce_invertibility=False).fit(disp=False)\n",
    "    fc1 = sarimax.get_forecast(steps=horizon, exog=X_te).predicted_mean\n",
    "\n",
    "    r_sarima.append(rmse(y_te, fc0)); r_sarimax.append(rmse(y_te, fc1))\n",
    "\n",
    "print(\"=== F3: Rolling-origin RMSE (12-aylık test, 5 kat) ===\")\n",
    "print(\"SARIMA  RMSE ort:\", round(np.mean(r_sarima),1), \" | katlar:\", [round(x,1) for x in r_sarima])\n",
    "print(\"SARIMAX RMSE ort:\", round(np.mean(r_sarimax),1), \"| katlar:\", [round(x,1) for x in r_sarimax])\n",
    "print(\"İyileşme (ort):\", round((np.mean(r_sarima)-np.mean(r_sarimax))/np.mean(r_sarima)*100,1), \"%\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d8826f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F4: Lag0'ı çıkar, yalnız lag>=1 exog'larla test (FIX: mstart)\n",
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from pmdarima import auto_arima\n",
    "\n",
    "CLEAN = Path(\"/kaggle/working/clean\")\n",
    "TARGET = \"new_users\"\n",
    "\n",
    "def mstart(s):\n",
    "    s = pd.to_datetime(s, errors=\"coerce\")\n",
    "    return s.dt.to_period(\"M\").dt.to_timestamp(how=\"start\")\n",
    "\n",
    "tr = pd.read_parquet(CLEAN / \"google_trends_long.parquet\")\n",
    "sc = pd.read_parquet(CLEAN / \"scratch_clean.parquet\")\n",
    "tr[\"date\"] = mstart(tr[\"date\"])\n",
    "sc[\"date\"] = mstart(sc[\"date\"])\n",
    "\n",
    "y = sc.set_index(\"date\")[TARGET].asfreq(\"MS\")\n",
    "corr = pd.read_csv(CLEAN / \"corr_lag_summary.csv\")\n",
    "\n",
    "cand = (corr.query(\"method=='pearson' and metric==@TARGET\")\n",
    "             .query(\"best_lag>=1 and best_lag<=6\")\n",
    "             .sort_values(\"r\", key=lambda s: s.abs(), ascending=False)\n",
    "             .drop_duplicates(\"term\").head(3))[[\"term\",\"best_lag\"]].values.tolist()\n",
    "\n",
    "tw = tr.pivot(index=\"date\", columns=[\"category\",\"term\"], values=\"interest\").asfreq(\"MS\").sort_index()\n",
    "tw.columns = [f\"{a}__{b}\" for a,b in tw.columns]\n",
    "X = pd.DataFrame(index=tw.index)\n",
    "for t,lag in cand:\n",
    "    X[f\"{t}__lag{lag}\"] = tw[t].rolling(3, min_periods=1).mean().shift(int(lag))\n",
    "\n",
    "TEST_H=24\n",
    "y_tr, y_te = y.iloc[:-TEST_H], y.iloc[-TEST_H:]\n",
    "X_tr, X_te = X.loc[y_tr.index], X.loc[y_te.index]\n",
    "\n",
    "a = auto_arima(y_tr, seasonal=True, m=12, information_criterion=\"aic\",\n",
    "               stepwise=True, suppress_warnings=True)\n",
    "order, so = a.order, a.seasonal_order\n",
    "\n",
    "m = SARIMAX(y_tr, exog=X_tr, order=order, seasonal_order=so,\n",
    "            enforce_stationarity=False, enforce_invertibility=False).fit(disp=False)\n",
    "fc = m.get_forecast(steps=TEST_H, exog=X_te).predicted_mean\n",
    "\n",
    "def metrics(y_true, y_pred):\n",
    "    e = (y_pred - y_true).astype(float)\n",
    "    return {\"MAE\": float(e.abs().mean()),\n",
    "            \"RMSE\": float(np.sqrt((e**2).mean())),\n",
    "            \"MAPE%\": float((e.abs()/y_true.replace(0,np.nan)).mean()*100)}\n",
    "\n",
    "print(\"=== F4: Lag≥1-only SARIMAX ===\")\n",
    "print(metrics(y_te, fc))\n",
    "print(\"Kullanılan exog:\", list(X.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0313d514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# G1: Platform terimleri ↔ Scratch metrikleri (lag=0 ve en iyi lag 0..6) — Pearson & Spearman\n",
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "CLEAN = Path(\"/kaggle/working/clean\")\n",
    "tr = pd.read_parquet(CLEAN / \"google_trends_long.parquet\")\n",
    "sc = pd.read_parquet(CLEAN / \"scratch_clean.parquet\")\n",
    "\n",
    "def mstart(s): \n",
    "    s = pd.to_datetime(s, errors=\"coerce\")\n",
    "    return s.dt.to_period(\"M\").dt.to_timestamp(how=\"start\")\n",
    "\n",
    "tr[\"date\"] = mstart(tr[\"date\"]); sc[\"date\"] = mstart(sc[\"date\"])\n",
    "tw = tr.pivot(index=\"date\", columns=[\"category\",\"term\"], values=\"interest\").sort_index()\n",
    "tw.columns = [f\"{a}__{b}\" for a,b in tw.columns]\n",
    "tw_ma = tw.rolling(3, min_periods=1).mean()\n",
    "\n",
    "sw = sc.set_index(\"date\")[[\"new_projects\",\"new_users\",\"new_comments\"]].sort_index()\n",
    "sw_ma = sw.rolling(3, min_periods=1).mean()\n",
    "\n",
    "platform_terms = [\n",
    "    \"platformlar_araclar__scratch\",\"platformlar_araclar__tynker\",\"platformlar_araclar__code_org\",\n",
    "    \"platformlar_araclar__blockly\",\"platformlar_araclar__kodable\",\"platformlar_araclar__bee_bot\",\n",
    "    \"platformlar_araclar__code_a_pillar\",\"platformlar_araclar__matatalab\"\n",
    "]\n",
    "platform_terms = [t for t in platform_terms if t in tw_ma.columns]\n",
    "\n",
    "def best_lag_corr(x, y, lags=range(0,7), method=\"pearson\"):\n",
    "    rows=[]\n",
    "    for k in lags:\n",
    "        ys = y.shift(k)\n",
    "        v = x.notna() & ys.notna()\n",
    "        if v.sum()>=12:\n",
    "            r = (x[v].corr(ys[v]) if method==\"pearson\" else x[v].rank().corr(ys[v].rank()))\n",
    "            rows.append((k, r, int(v.sum())))\n",
    "    if not rows: \n",
    "        return None\n",
    "    best = max(rows, key=lambda t: abs(t[1]))\n",
    "    return {\"lag\":best[0], \"r\":best[1], \"n\":best[2]}\n",
    "\n",
    "out=[]\n",
    "for term in platform_terms:\n",
    "    x = tw_ma[term]\n",
    "    for m in sw_ma.columns:\n",
    "        y = sw_ma[m]\n",
    "        # lag 0\n",
    "        v = x.notna() & y.notna()\n",
    "        r0_p = x[v].corr(y[v]); r0_s = x[v].rank().corr(y[v].rank()); n0=int(v.sum())\n",
    "        # en iyi 0..6\n",
    "        bp = best_lag_corr(x,y,method=\"pearson\"); bs = best_lag_corr(x,y,method=\"spearman\")\n",
    "        out.append({\n",
    "            \"term\": term.split(\"__\",1)[1],\n",
    "            \"metric\": m,\n",
    "            \"r0_pearson\": round(r0_p,3), \"r0_spearman\": round(r0_s,3), \"n0\": n0,\n",
    "            \"best_lag_p\": (None if not bp else bp[\"lag\"]),\n",
    "            \"best_r_p\":   (None if not bp else round(bp[\"r\"],3)),\n",
    "            \"best_lag_s\": (None if not bs else bs[\"lag\"]),\n",
    "            \"best_r_s\":   (None if not bs else round(bs[\"r\"],3)),\n",
    "            \"n_best\":     (None if not bp else bp[\"n\"])\n",
    "        })\n",
    "platform_compare = pd.DataFrame(out).sort_values([\"metric\",\"best_r_s\"], ascending=[True,False])\n",
    "print(\"=== PLATFORM KARŞILAŞTIRMA (lag=0 ve en iyi 0..6) ===\")\n",
    "print(platform_compare.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2804902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# G2: new_users, new_projects, new_comments için sNaive / SARIMA / SARIMAX kıyas tablosu\n",
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "from pmdarima import auto_arima\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "CLEAN = Path(\"/kaggle/working/clean\")\n",
    "tr = pd.read_parquet(CLEAN / \"google_trends_long.parquet\")\n",
    "sc = pd.read_parquet(CLEAN / \"scratch_clean.parquet\")\n",
    "corr = pd.read_csv(CLEAN / \"corr_lag_summary.csv\")\n",
    "\n",
    "def mstart(s): \n",
    "    s = pd.to_datetime(s, errors=\"coerce\")\n",
    "    return s.dt.to_period(\"M\").dt.to_timestamp(how=\"start\")\n",
    "\n",
    "tr[\"date\"] = mstart(tr[\"date\"]); sc[\"date\"] = mstart(sc[\"date\"])\n",
    "\n",
    "tw = tr.pivot(index=\"date\", columns=[\"category\",\"term\"], values=\"interest\").asfreq(\"MS\").sort_index()\n",
    "tw.columns = [f\"{a}__{b}\" for a,b in tw.columns]\n",
    "\n",
    "def build_exog_for_target(target, k=3):\n",
    "    c = (corr.query(\"method=='pearson' and metric==@target\")\n",
    "                 .query(\"best_lag>=0 and best_lag<=6\")\n",
    "                 .sort_values(\"r\", key=lambda s: s.abs(), ascending=False)\n",
    "                 .drop_duplicates(\"term\").head(k))[[\"term\",\"best_lag\"]].values.tolist()\n",
    "    X = pd.DataFrame(index=tw.index)\n",
    "    for t,lag in c:\n",
    "        X[f\"{t}__lag{lag}\"] = tw[t].rolling(3, min_periods=1).mean().shift(int(lag))\n",
    "    return X, [f\"{t}__lag{lag}\" for t,lag in c]\n",
    "\n",
    "def metrics(y_true, y_pred):\n",
    "    e = (y_pred - y_true).astype(float)\n",
    "    return {\"MAE\": float(e.abs().mean()),\n",
    "            \"RMSE\": float(np.sqrt((e**2).mean())),\n",
    "            \"MAPE%\": float((e.abs()/y_true.replace(0,np.nan)).mean()*100)}\n",
    "\n",
    "rows=[]\n",
    "for TARGET in [\"new_users\",\"new_projects\",\"new_comments\"]:\n",
    "    y = sc.set_index(\"date\")[TARGET].asfreq(\"MS\")\n",
    "    TEST_H=24\n",
    "    y_tr, y_te = y.iloc[:-TEST_H], y.iloc[-TEST_H:]\n",
    "    # sNaive\n",
    "    snaive_fc = y.shift(12).iloc[-TEST_H:]\n",
    "    m_snaive = metrics(y_te, snaive_fc)\n",
    "    # SARIMA düzeni\n",
    "    a = auto_arima(y_tr, seasonal=True, m=12, information_criterion=\"aic\",\n",
    "                   stepwise=True, suppress_warnings=True)\n",
    "    order, so = a.order, a.seasonal_order\n",
    "    sarima = SARIMAX(y_tr, order=order, seasonal_order=so,\n",
    "                     enforce_stationarity=False, enforce_invertibility=False).fit(disp=False)\n",
    "    base_fc = sarima.get_forecast(steps=TEST_H).predicted_mean\n",
    "    m_sarima = metrics(y_te, base_fc)\n",
    "    # SARIMAX (ilk 3 exog)\n",
    "    X, used = build_exog_for_target(TARGET, k=3)\n",
    "    X_tr, X_te = X.loc[y_tr.index], X.loc[y_te.index]\n",
    "    try:\n",
    "        sarimax = SARIMAX(y_tr, exog=X_tr, order=order, seasonal_order=so,\n",
    "                          enforce_stationarity=False, enforce_invertibility=False).fit(disp=False)\n",
    "        ex_fc = sarimax.get_forecast(steps=TEST_H, exog=X_te).predicted_mean\n",
    "        m_sarimax = metrics(y_te, ex_fc)\n",
    "    except Exception as e:\n",
    "        m_sarimax = {\"MAE\":np.nan,\"RMSE\":np.nan,\"MAPE%\":np.nan}; used = [f\"ERROR: {e}\"]\n",
    "    rows.append({\n",
    "        \"target\": TARGET,\n",
    "        \"sNaive_RMSE\": round(m_snaive[\"RMSE\"],1), \"SARIMA_RMSE\": round(m_sarima[\"RMSE\"],1), \"SARIMAX_RMSE\": round(m_sarimax[\"RMSE\"],1),\n",
    "        \"sNaive_MAE\": round(m_snaive[\"MAE\"],1), \"SARIMA_MAE\": round(m_sarima[\"MAE\"],1), \"SARIMAX_MAE\": round(m_sarimax[\"MAE\"],1),\n",
    "        \"sNaive_MAPE%\": round(m_snaive[\"MAPE%\"],2), \"SARIMA_MAPE%\": round(m_sarima[\"MAPE%\"],2), \"SARIMAX_MAPE%\": round(m_sarimax[\"MAPE%\"],2),\n",
    "        \"SARIMAX_exog\": used\n",
    "    })\n",
    "tbl = pd.DataFrame(rows)\n",
    "print(\"=== ÜÇ HEDEF İÇİN TOPLU MODELLER ===\")\n",
    "print(tbl.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2c9c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# G2-EXTRA: new_comments — sNaive vs SARIMA vs SARIMAX (lag0 sabit terimler)\n",
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "from pmdarima import auto_arima\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "CLEAN = Path(\"/kaggle/working/clean\")\n",
    "\n",
    "def mstart(s):\n",
    "    s = pd.to_datetime(s, errors=\"coerce\")\n",
    "    return s.dt.to_period(\"M\").dt.to_timestamp(how=\"start\")\n",
    "\n",
    "# --- verileri yükle ---\n",
    "tr = pd.read_parquet(CLEAN / \"google_trends_long.parquet\")   # date, category, term, interest\n",
    "sc = pd.read_parquet(CLEAN / \"scratch_clean.parquet\")        # date, new_projects, new_users, new_comments\n",
    "tr[\"date\"] = mstart(tr[\"date\"])\n",
    "sc[\"date\"] = mstart(sc[\"date\"])\n",
    "\n",
    "# hedef seri\n",
    "TARGET = \"new_comments\"\n",
    "y = sc.set_index(\"date\")[TARGET].asfreq(\"MS\")\n",
    "TEST_H = 24\n",
    "y_tr, y_te = y.iloc[:-TEST_H], y.iloc[-TEST_H:]\n",
    "\n",
    "# sNaive: geçen yılın aynı ayı\n",
    "snaive_fc = y.shift(12).iloc[-TEST_H:]\n",
    "\n",
    "# SARIMA düzeni\n",
    "a = auto_arima(y_tr, seasonal=True, m=12, information_criterion=\"aic\",\n",
    "               stepwise=True, suppress_warnings=True, error_action=\"ignore\")\n",
    "order, so = a.order, a.seasonal_order\n",
    "sarima = SARIMAX(y_tr, order=order, seasonal_order=so,\n",
    "                 enforce_stationarity=False, enforce_invertibility=False).fit(disp=False)\n",
    "sarima_fc = sarima.get_forecast(steps=TEST_H).predicted_mean\n",
    "\n",
    "# --- lag=0 sabit terimlerle exog kur (G1'e göre en güçlü lag0: scratch, blockly, bee_bot) ---\n",
    "tw = tr.pivot(index=\"date\", columns=[\"category\",\"term\"], values=\"interest\").asfreq(\"MS\").sort_index()\n",
    "tw.columns = [f\"{a}__{b}\" for a,b in tw.columns]\n",
    "\n",
    "lag0_terms = [\n",
    "    \"platformlar_araclar__scratch\",\n",
    "    \"platformlar_araclar__blockly\",\n",
    "    \"platformlar_araclar__bee_bot\",\n",
    "]\n",
    "lag0_terms = [t for t in lag0_terms if t in tw.columns]  # güvenli filtre\n",
    "\n",
    "X = pd.DataFrame(index=tw.index)\n",
    "for t in lag0_terms:\n",
    "    X[f\"{t}__lag0\"] = tw[t].rolling(3, min_periods=1).mean()   # 3M MA, lag=0\n",
    "\n",
    "X_tr, X_te = X.loc[y_tr.index], X.loc[y_te.index]\n",
    "\n",
    "sarimax = SARIMAX(y_tr, exog=X_tr, order=order, seasonal_order=so,\n",
    "                  enforce_stationarity=False, enforce_invertibility=False).fit(disp=False)\n",
    "sarimax_fc = sarimax.get_forecast(steps=TEST_H, exog=X_te).predicted_mean\n",
    "\n",
    "def metrics(y_true, y_pred):\n",
    "    e = (y_pred - y_true).astype(float)\n",
    "    return {\n",
    "        \"MAE\": float(e.abs().mean()),\n",
    "        \"RMSE\": float(np.sqrt((e**2).mean())),\n",
    "        \"MAPE%\": float((e.abs() / y_true.replace(0, np.nan)).mean() * 100),\n",
    "    }\n",
    "\n",
    "print(\"=== G2-EXTRA: new_comments — sNaive vs SARIMA vs SARIMAX(lag0) ===\")\n",
    "print(\"Kullanılan lag0 exog:\", lag0_terms)\n",
    "print(\"sNaive:\", metrics(y_te, snaive_fc))\n",
    "print(\"SARIMA:\", metrics(y_te, sarima_fc))\n",
    "print(\"SARIMAX(lag0):\", metrics(y_te, sarimax_fc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c1c511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adım 5 / R1: Otomatik rapor üretimi (Markdown + PNG görseller)\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from pmdarima import auto_arima\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "\n",
    "\n",
    "plt.rcParams[\"figure.dpi\"] = 120\n",
    "\n",
    "CLEAN = Path(\"/kaggle/working/clean\")\n",
    "OUT = Path(\"/kaggle/working/report_assets\")\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------------------------\n",
    "# 0) Yüklemeler ve yardımcılar\n",
    "# ---------------------------\n",
    "def mstart(s):\n",
    "    s = pd.to_datetime(s, errors=\"coerce\")\n",
    "    return s.dt.to_period(\"M\").dt.to_timestamp(how=\"start\")\n",
    "\n",
    "def minmax01(s):\n",
    "    s = s.astype(float)\n",
    "    lo, hi = s.min(), s.max()\n",
    "    return (s - lo) / (hi - lo) if hi > lo else s*0 + 0.5\n",
    "\n",
    "def metrics(y_true, y_pred):\n",
    "    e = (y_pred - y_true).astype(float)\n",
    "    return {\"MAE\": float(e.abs().mean()),\n",
    "            \"RMSE\": float(np.sqrt((e**2).mean())),\n",
    "            \"MAPE%\": float((e.abs()/y_true.replace(0,np.nan)).mean()*100)}\n",
    "\n",
    "tr = pd.read_parquet(CLEAN / \"google_trends_long.parquet\")\n",
    "sc = pd.read_parquet(CLEAN / \"scratch_clean.parquet\")\n",
    "corr_sum = pd.read_csv(CLEAN / \"corr_lag_summary.csv\")\n",
    "# opsiyonel\n",
    "try:\n",
    "    granger_seas = pd.read_csv(CLEAN / \"granger_seasonal_summary.csv\")\n",
    "except FileNotFoundError:\n",
    "    granger_seas = pd.DataFrame()\n",
    "\n",
    "tr[\"date\"] = mstart(tr[\"date\"])\n",
    "sc[\"date\"] = mstart(sc[\"date\"])\n",
    "\n",
    "# Wide görünümler\n",
    "tw = tr.pivot(index=\"date\", columns=[\"category\",\"term\"], values=\"interest\").asfreq(\"MS\").sort_index()\n",
    "tw.columns = [f\"{a}__{b}\" for a,b in tw.columns]\n",
    "sw = sc.set_index(\"date\")[[\"new_projects\",\"new_users\",\"new_comments\"]].asfreq(\"MS\").sort_index()\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Grafikler: EDA\n",
    "# ---------------------------\n",
    "# 1A: Kategori ortalamaları (3M MA)\n",
    "cat_mean = (tr.groupby([\"date\",\"category\"])[\"interest\"].mean()\n",
    "              .unstack(\"category\").sort_index().rolling(3, min_periods=1).mean())\n",
    "plt.figure(figsize=(10,4))\n",
    "for c in cat_mean.columns:\n",
    "    plt.plot(cat_mean.index, cat_mean[c], label=c)\n",
    "plt.title(\"Google Trends — Kategori Ortalamaları (3M MA)\")\n",
    "plt.xlabel(\"Tarih\"); plt.ylabel(\"İlgi (0–100)\"); plt.legend(loc=\"upper left\", ncol=2, fontsize=8)\n",
    "plt.tight_layout(); plt.savefig(OUT / \"eda_cat_means.png\"); plt.close()\n",
    "\n",
    "# 1B: Scratch metrikleri (3M MA)\n",
    "sw_ma = sw.rolling(3, min_periods=1).mean()\n",
    "plt.figure(figsize=(10,4))\n",
    "for c in sw_ma.columns:\n",
    "    plt.plot(sw_ma.index, sw_ma[c], label=c)\n",
    "plt.title(\"Scratch metrikleri (3M MA)\")\n",
    "plt.xlabel(\"Tarih\"); plt.ylabel(\"Sayı\"); plt.legend(loc=\"upper left\")\n",
    "plt.tight_layout(); plt.savefig(OUT / \"eda_scratch_ma.png\"); plt.close()\n",
    "\n",
    "# 1C: 'scratch' trendi vs new_users (normalize 0–100, 3M MA)\n",
    "x = tw[\"platformlar_araclar__scratch\"].rolling(3, min_periods=1).mean()\n",
    "y = sw[\"new_users\"].rolling(3, min_periods=1).mean()\n",
    "both = pd.concat([minmax01(x).rename(\"scratch_trend_norm\"),\n",
    "                  minmax01(y).rename(\"new_users_norm\")], axis=1).dropna()*100\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(both.index, both[\"scratch_trend_norm\"], label=\"scratch trend (norm)\")\n",
    "plt.plot(both.index, both[\"new_users_norm\"], label=\"new_users (norm)\")\n",
    "plt.title(\"Scratch trend vs new_users (normalize 0–100, 3M MA)\")\n",
    "plt.xlabel(\"Tarih\"); plt.ylabel(\"Normalize (0–100)\"); plt.legend(loc=\"upper left\")\n",
    "plt.tight_layout(); plt.savefig(OUT / \"eda_scratch_vs_users.png\"); plt.close()\n",
    "\n",
    "# 1D: Lag eğrisi (scratch vs new_users) [-12..+12]\n",
    "def corr_with_lags(x, y, lags=range(-12,13), method=\"pearson\"):\n",
    "    rows=[]\n",
    "    for k in lags:\n",
    "        ys = y.shift(k)\n",
    "        v = x.notna() & ys.notna()\n",
    "        if v.sum()>=12:\n",
    "            r = (x[v].corr(ys[v]) if method==\"pearson\" else x[v].rank().corr(ys[v].rank()))\n",
    "            rows.append((k, r))\n",
    "    return pd.DataFrame(rows, columns=[\"lag\",\"r\"])\n",
    "\n",
    "x_ma = x.copy(); y_ma = y.copy()\n",
    "dfp = corr_with_lags(x_ma, y_ma, method=\"pearson\")\n",
    "dfs = corr_with_lags(x_ma, y_ma, method=\"spearman\")\n",
    "plt.figure(figsize=(8,3.5))\n",
    "plt.plot(dfp[\"lag\"], dfp[\"r\"], label=\"pearson\")\n",
    "plt.plot(dfs[\"lag\"], dfs[\"r\"], label=\"spearman\", linestyle=\"--\")\n",
    "plt.axvline(0, color=\"k\", alpha=.2); plt.axvline(12, color=\"k\", alpha=.1); plt.axvline(-12, color=\"k\", alpha=.1)\n",
    "plt.title(\"Lag eğrisi: scratch trend ↔ new_users\")\n",
    "plt.xlabel(\"Lag (ay)  |  +: Trend önce\"); plt.ylabel(\"r\"); plt.legend()\n",
    "plt.tight_layout(); plt.savefig(OUT / \"lag_curve_scratch_users.png\"); plt.close()\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Tablolar: En güçlü korelasyonlar + Granger\n",
    "# ---------------------------\n",
    "top_corr = corr_sum.sort_values(\"r\", key=lambda s: s.abs(), ascending=False).head(12)\n",
    "top_corr_path = OUT / \"top_corr.csv\"; top_corr.to_csv(top_corr_path, index=False)\n",
    "\n",
    "if not granger_seas.empty:\n",
    "    top_granger = granger_seas.sort_values(\"best_p\").head(10)\n",
    "    top_granger_path = OUT / \"top_granger.csv\"; top_granger.to_csv(top_granger_path, index=False)\n",
    "else:\n",
    "    top_granger_path = None\n",
    "\n",
    "# ---------------------------\n",
    "# 3) Tahmin grafiği (new_users) — baseline vs exog_best\n",
    "# ---------------------------\n",
    "fc_tbl = pd.read_csv(CLEAN / \"forecast_new_users.csv\", index_col=0, parse_dates=True)\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(pd.concat([sw[\"new_users\"].iloc[:-24], fc_tbl[\"actual\"]]), label=\"gerçek\")\n",
    "plt.plot(fc_tbl.index, fc_tbl[\"baseline\"], label=\"baseline (SARIMA)\")\n",
    "if fc_tbl[\"exog_best\"].notna().any():\n",
    "    plt.plot(fc_tbl.index, fc_tbl[\"exog_best\"], label=\"exog_best (SARIMAX)\")\n",
    "plt.title(\"Tahmin — new_users (son 24 ay test)\")\n",
    "plt.xlabel(\"Tarih\"); plt.ylabel(\"new_users\"); plt.legend(loc=\"upper left\")\n",
    "plt.tight_layout(); plt.savefig(OUT / \"forecast_new_users.png\"); plt.close()\n",
    "\n",
    "# ---------------------------\n",
    "# 4) Platform karşılaştırma (lag=0 ve en iyi 0..6) — tablo CSV\n",
    "# ---------------------------\n",
    "platform_terms = [\n",
    "    \"platformlar_araclar__scratch\",\"platformlar_araclar__tynker\",\"platformlar_araclar__code_org\",\n",
    "    \"platformlar_araclar__blockly\",\"platformlar_araclar__kodable\",\"platformlar_araclar__bee_bot\",\n",
    "    \"platformlar_araclar__code_a_pillar\",\"platformlar_araclar__matatalab\"\n",
    "]\n",
    "platform_terms = [t for t in platform_terms if t in tw.columns]\n",
    "\n",
    "def best_lag_corr(x, y, lags=range(0,7), method=\"pearson\"):\n",
    "    rows=[]\n",
    "    for k in lags:\n",
    "        ys = y.shift(k)\n",
    "        v = x.notna() & ys.notna()\n",
    "        if v.sum()>=12:\n",
    "            r = (x[v].corr(ys[v]) if method==\"pearson\" else x[v].rank().corr(ys[v].rank()))\n",
    "            rows.append((k, r, int(v.sum())))\n",
    "    if not rows: \n",
    "        return None\n",
    "    best = max(rows, key=lambda t: abs(t[1]))\n",
    "    return {\"lag\":best[0], \"r\":best[1], \"n\":best[2]}\n",
    "\n",
    "tw_ma = tw.rolling(3, min_periods=1).mean(); sw_ma = sw.rolling(3, min_periods=1).mean()\n",
    "out=[]\n",
    "for term in platform_terms:\n",
    "    x = tw_ma[term]\n",
    "    for m in sw_ma.columns:\n",
    "        y = sw_ma[m]\n",
    "        v = x.notna() & y.notna(); n0=int(v.sum())\n",
    "        r0p = x[v].corr(y[v]); r0s = x[v].rank().corr(y[v].rank())\n",
    "        bp = best_lag_corr(x,y,method=\"pearson\"); bs = best_lag_corr(x,y,method=\"spearman\")\n",
    "        out.append({\n",
    "            \"term\": term.split(\"__\",1)[1],\n",
    "            \"metric\": m,\n",
    "            \"r0_pearson\": round(r0p,3), \"r0_spearman\": round(r0s,3), \"n0\": n0,\n",
    "            \"best_lag_p\": (None if not bp else bp[\"lag\"]),\n",
    "            \"best_r_p\":   (None if not bp else round(bp[\"r\"],3)),\n",
    "            \"best_lag_s\": (None if not bs else bs[\"lag\"]),\n",
    "            \"best_r_s\":   (None if not bs else round(bs[\"r\"],3)),\n",
    "            \"n_best\":     (None if not bp else bp[\"n\"])\n",
    "        })\n",
    "platform_compare = pd.DataFrame(out).sort_values([\"metric\",\"best_r_s\"], ascending=[True,False])\n",
    "platform_compare_path = OUT / \"platform_compare.csv\"\n",
    "platform_compare.to_csv(platform_compare_path, index=False)\n",
    "\n",
    "# ---------------------------\n",
    "# 5) Üç hedef için toplu model kıyası (sNaive vs SARIMA vs SARIMAX)\n",
    "# ---------------------------\n",
    "def build_exog_for_target(target, k=3):\n",
    "    c = (corr_sum.query(\"method=='pearson' and metric==@target\")\n",
    "                 .query(\"best_lag>=0 and best_lag<=6\")\n",
    "                 .sort_values(\"r\", key=lambda s: s.abs(), ascending=False)\n",
    "                 .drop_duplicates(\"term\").head(k))[[\"term\",\"best_lag\"]].values.tolist()\n",
    "    X = pd.DataFrame(index=tw.index)\n",
    "    used=[]\n",
    "    for t,lag in c:\n",
    "        col = f\"{t}__lag{int(lag)}\"\n",
    "        X[col] = tw[t].rolling(3, min_periods=1).mean().shift(int(lag))\n",
    "        used.append(col)\n",
    "    return X, used\n",
    "\n",
    "rows=[]\n",
    "for TARGET in [\"new_users\",\"new_projects\",\"new_comments\"]:\n",
    "    y = sw[TARGET]\n",
    "    TEST_H=24\n",
    "    y_tr, y_te = y.iloc[:-TEST_H], y.iloc[-TEST_H:]\n",
    "    snaive_fc = y.shift(12).iloc[-TEST_H:]\n",
    "    m_snaive = metrics(y_te, snaive_fc)\n",
    "\n",
    "    a = auto_arima(y_tr, seasonal=True, m=12, information_criterion=\"aic\",\n",
    "                   stepwise=True, suppress_warnings=True, error_action=\"ignore\")\n",
    "    order, so = a.order, a.seasonal_order\n",
    "    sarima = SARIMAX(y_tr, order=order, seasonal_order=so,\n",
    "                     enforce_stationarity=False, enforce_invertibility=False).fit(disp=False)\n",
    "    base_fc = sarima.get_forecast(steps=TEST_H).predicted_mean\n",
    "    m_sarima = metrics(y_te, base_fc)\n",
    "\n",
    "    X, used = build_exog_for_target(TARGET, k=3)\n",
    "    X_tr, X_te = X.loc[y_tr.index], X.loc[y_te.index]\n",
    "    try:\n",
    "        sarimax = SARIMAX(y_tr, exog=X_tr, order=order, seasonal_order=so,\n",
    "                          enforce_stationarity=False, enforce_invertibility=False).fit(disp=False)\n",
    "        ex_fc = sarimax.get_forecast(steps=TEST_H, exog=X_te).predicted_mean\n",
    "        m_sarimax = metrics(y_te, ex_fc)\n",
    "    except Exception as e:\n",
    "        m_sarimax = {\"MAE\":np.nan,\"RMSE\":np.nan,\"MAPE%\":np.nan}\n",
    "        used = [f\"ERROR: {e}\"]\n",
    "    rows.append({\n",
    "        \"target\": TARGET,\n",
    "        \"sNaive_RMSE\": round(m_snaive[\"RMSE\"],1), \"SARIMA_RMSE\": round(m_sarima[\"RMSE\"],1), \"SARIMAX_RMSE\": round(m_sarimax[\"RMSE\"],1),\n",
    "        \"sNaive_MAE\": round(m_snaive[\"MAE\"],1), \"SARIMA_MAE\": round(m_sarima[\"MAE\"],1), \"SARIMAX_MAE\": round(m_sarimax[\"MAE\"],1),\n",
    "        \"sNaive_MAPE%\": round(m_snaive[\"MAPE%\"],2), \"SARIMA_MAPE%\": round(m_sarima[\"MAPE%\"],2), \"SARIMAX_MAPE%\": round(m_sarimax[\"MAPE%\"],2),\n",
    "        \"SARIMAX_exog\": used\n",
    "    })\n",
    "model_tbl = pd.DataFrame(rows)\n",
    "model_tbl_path = OUT / \"models_all_targets.csv\"\n",
    "model_tbl.to_csv(model_tbl_path, index=False)\n",
    "\n",
    "# ---------------------------\n",
    "# 6) Artık tanı (new_users, en iyi 3 exog)\n",
    "# ---------------------------\n",
    "cand = (corr_sum.query(\"method=='pearson' and metric=='new_users'\")\n",
    "             .query(\"best_lag>=0 and best_lag<=6\")\n",
    "             .sort_values(\"r\", key=lambda s: s.abs(), ascending=False)\n",
    "             .drop_duplicates(\"term\").head(3))[[\"term\",\"best_lag\"]].values.tolist()\n",
    "X = pd.DataFrame(index=tw.index)\n",
    "for t,lag in cand:\n",
    "    X[f\"{t}__lag{int(lag)}\"] = tw[t].rolling(3, min_periods=1).mean().shift(int(lag))\n",
    "\n",
    "y = sw[\"new_users\"]; TEST_H=24\n",
    "y_tr, y_te = y.iloc[:-TEST_H], y.iloc[-TEST_H:]\n",
    "X_tr, X_te = X.loc[y_tr.index], X.loc[y_te.index]\n",
    "a = auto_arima(y_tr, seasonal=True, m=12, information_criterion=\"aic\",\n",
    "               stepwise=True, suppress_warnings=True, error_action=\"ignore\")\n",
    "order, so = a.order, a.seasonal_order\n",
    "m = SARIMAX(y_tr, exog=X_tr, order=order, seasonal_order=so,\n",
    "            enforce_stationarity=False, enforce_invertibility=False).fit(disp=False)\n",
    "resid = pd.Series(m.filter_results.standardized_forecasts_error[0], index=y_tr.index).dropna()\n",
    "lb = acorr_ljungbox(resid, lags=[12,24], return_df=True).round(4)\n",
    "lb_path = OUT / \"ljungbox_new_users.csv\"; lb.to_csv(lb_path, index=False)\n",
    "\n",
    "# ---------------------------\n",
    "# 7) Markdown dosyasını yaz\n",
    "# ---------------------------\n",
    "md = f\"\"\"# Google Trends × Scratch Analizi — Rapor\n",
    "\n",
    "**Veri setleri:** Google Trends (kategoriler/terimler) ve Scratch (new_users, new_projects, new_comments).  \n",
    "**Amaç:** Trends'teki ilgi ile Scratch kullanım metrikleri arasındaki ilişkiyi incelemek; tahminlerde exog katkısını test etmek.\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Betimsel Analiz (EDA)\n",
    "- Kategorilerin yıllar içindeki ortalamaları (3M MA):  \n",
    "  ![](report_assets/eda_cat_means.png)\n",
    "- Scratch metrikleri (3M MA):  \n",
    "  ![](report_assets/eda_scratch_ma.png)\n",
    "- 'scratch' arama ilgisi × `new_users` (normalize 0–100):  \n",
    "  ![](report_assets/eda_scratch_vs_users.png)\n",
    "\n",
    "**Not:** Güçlü 12-ay sezonsallığı gözleniyor.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) İlişki Analizi\n",
    "- Lag eğrisi ('scratch' ↔ `new_users`):  \n",
    "  ![](report_assets/lag_curve_scratch_users.png)\n",
    "\n",
    "- **En güçlü 12 ilişki (lag taraması, Pearson/Spearman):**  \n",
    "  (dosya: `report_assets/top_corr.csv`)\n",
    "\n",
    "- **Mevsimsel farkla Granger (özet):**  \n",
    "  {('dosya: report_assets/top_granger.csv' if top_granger_path else 'uygun anlamlı sonuç yok veya dosya üretilmedi')}\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Platform Karşılaştırması\n",
    "Tablo: `report_assets/platform_compare.csv`  \n",
    "(Lag=0 korelasyonları ve **0..6 ay** içinde en yüksek korelasyon lagı.)\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Tahmin (Son 24 ay test)\n",
    "- `new_users` tahmin grafiği:  \n",
    "  ![](report_assets/forecast_new_users.png)\n",
    "\n",
    "- **Üç hedef için kıyas** (sNaive vs SARIMA vs SARIMAX):  \n",
    "  Tablo: `report_assets/models_all_targets.csv`\n",
    "\n",
    "**Özet:** SARIMAX, SARIMA'ya göre belirgin iyileşme sağlasa da **sNaive** (y_{ '{t-12}' }) çoğu hedefte en düşük hatayı verir.\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Artık Tanı (new_users, SARIMAX)\n",
    "- Ljung–Box p-değerleri (12, 24): dosya `report_assets/ljungbox_new_users.csv`  \n",
    "  (Yüksek p-değeri → artıklar beyaz gürültüye yakın.)\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Sonuçlar ve Öneriler\n",
    "- **İlişki:** Yüksek korelasyonlar büyük ölçüde **eşzamanlı** ve **yıllık sezonsallık** kaynaklıdır. Kısa vadeli lider sinyal adayları: `coding_in_school (≈+4 ay)` ve sınırlı olarak `matatalab (≈+6 ay)`.\n",
    "- **Tahmin:** Exog'lar (Trends) SARIMA'yı iyileştirir; fakat sNaive çoğu hedefte daha iyi kalır.\n",
    "- **Uygulama:** Üretimde **sNaive**'i baseline yapın; üstüne **residual-correction** (sNaive artıkları ≈ exog ile modelleme) deneyin. Operasyonel ileri tahmin için **lag≥1** exog'lar kullanılmalıdır.\n",
    "\"\"\"\n",
    "\n",
    "(Path(\"/kaggle/working\") / \"report.md\").write_text(md, encoding=\"utf-8\")\n",
    "\n",
    "print(\"=== ADIM 5 / R1 TAMAMLANDI ===\")\n",
    "print(\"Rapor:\", \"/kaggle/working/report.md\")\n",
    "print(\"Varlık klasörü:\", str(OUT))\n",
    "print(\"Ek tablolar:\")\n",
    "print(\"-\", top_corr_path)\n",
    "if top_granger_path: print(\"-\", top_granger_path)\n",
    "print(\"-\", platform_compare_path)\n",
    "print(\"-\", model_tbl_path)\n",
    "print(\"-\", lb_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c365c6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2: STL decomposition (period=12) görselleri\n",
    "import pandas as pd, matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from pathlib import Path\n",
    "\n",
    "plt.rcParams[\"figure.dpi\"] = 120\n",
    "\n",
    "CLEAN = Path(\"/kaggle/working/clean\")\n",
    "OUT   = Path(\"/kaggle/working/report_assets\")\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "sc = pd.read_parquet(CLEAN / \"scratch_clean.parquet\")\n",
    "sc[\"date\"] = pd.to_datetime(sc[\"date\"]).dt.to_period(\"M\").dt.to_timestamp(how=\"start\")\n",
    "sw = sc.set_index(\"date\")[[\"new_users\",\"new_projects\",\"new_comments\"]].asfreq(\"MS\")\n",
    "\n",
    "for col in sw.columns:\n",
    "    y = sw[col].dropna()\n",
    "    stl = STL(y, period=12, robust=True).fit()\n",
    "    fig = stl.plot()\n",
    "    fig.set_size_inches(10,6)\n",
    "    fig.suptitle(f\"STL Decomposition — {col} (period=12)\", fontsize=12, y=1.02)\n",
    "    (OUT / f\"stl_{col}.png\").unlink(missing_ok=True)\n",
    "    fig.savefig(OUT / f\"stl_{col}.png\", bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "print(\"STL görselleri kaydedildi:\", [f\"stl_{c}.png\" for c in sw.columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d15ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R3: sNaive + Residual-Correction (lag>=1 exog) — üç hedef için\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "plt.rcParams[\"figure.dpi\"] = 120\n",
    "\n",
    "CLEAN = Path(\"/kaggle/working/clean\")\n",
    "OUT   = Path(\"/kaggle/working/report_assets\")\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def mstart(s):\n",
    "    s = pd.to_datetime(s, errors=\"coerce\")\n",
    "    return s.dt.to_period(\"M\").dt.to_timestamp(how=\"start\")\n",
    "\n",
    "def metrics(y_true, y_pred):\n",
    "    e = (y_pred - y_true).astype(float)\n",
    "    return {\"MAE\": float(e.abs().mean()),\n",
    "            \"RMSE\": float(np.sqrt((e**2).mean())),\n",
    "            \"MAPE%\": float((e.abs()/y_true.replace(0,np.nan)).mean()*100)}\n",
    "\n",
    "# veriler\n",
    "tr = pd.read_parquet(CLEAN / \"google_trends_long.parquet\")\n",
    "sc = pd.read_parquet(CLEAN / \"scratch_clean.parquet\")\n",
    "corr = pd.read_csv(CLEAN / \"corr_lag_summary.csv\")\n",
    "\n",
    "tr[\"date\"] = mstart(tr[\"date\"])\n",
    "sc[\"date\"] = mstart(sc[\"date\"])\n",
    "\n",
    "tw = tr.pivot(index=\"date\", columns=[\"category\",\"term\"], values=\"interest\").asfreq(\"MS\").sort_index()\n",
    "tw.columns = [f\"{a}__{b}\" for a,b in tw.columns]\n",
    "sw = sc.set_index(\"date\")[[\"new_users\",\"new_projects\",\"new_comments\"]].asfreq(\"MS\").sort_index()\n",
    "\n",
    "TEST_H = 24\n",
    "rows=[]\n",
    "plots_done=False\n",
    "\n",
    "for TARGET in sw.columns:\n",
    "    y = sw[TARGET]\n",
    "    y_train, y_test = y.iloc[:-TEST_H], y.iloc[-TEST_H:]\n",
    "\n",
    "    # sNaive baseline\n",
    "    snaive_train = y_train.shift(12)\n",
    "    snaive_test  = y.shift(12).iloc[-TEST_H:]\n",
    "\n",
    "    # Residual seri (train dönemi için)\n",
    "    resid_train = (y_train - snaive_train).dropna()\n",
    "\n",
    "    # Exog seçimi: lag in [1..6], en güçlü 3 terim (Pearson)\n",
    "    sel = (corr.query(\"method=='pearson' and metric==@TARGET\")\n",
    "                .query(\"best_lag>=1 and best_lag<=6\")\n",
    "                .sort_values(\"r\", key=lambda s: s.abs(), ascending=False)\n",
    "                .drop_duplicates(\"term\").head(3))\n",
    "    exog_specs = [(t, int(l)) for t,l in sel[[\"term\",\"best_lag\"]].itertuples(index=False, name=None)]\n",
    "\n",
    "    # fallback: hiç bulunamazsa scratch lag1 ekle\n",
    "    if not exog_specs and \"platformlar_araclar__scratch\" in tw.columns:\n",
    "        exog_specs = [(\"platformlar_araclar__scratch\", 1)]\n",
    "\n",
    "    # Exog matrisi\n",
    "    X_all = pd.DataFrame(index=tw.index)\n",
    "    for t,lag in exog_specs:\n",
    "        X_all[f\"{t}__lag{lag}\"] = tw[t].rolling(3, min_periods=1).mean().shift(lag)\n",
    "\n",
    "    # Train/test eşitleme (resid_train indeksine göre)\n",
    "    X_tr = X_all.loc[resid_train.index]\n",
    "    X_te = X_all.loc[y_test.index]\n",
    "\n",
    "    # Pipeline: scale + ridge\n",
    "    pipe = Pipeline([(\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "                     (\"ridge\", Ridge(alpha=1.0, random_state=0))])\n",
    "    # fit sadece ortak NaN olmayanlarda\n",
    "    v = X_tr.notna().all(axis=1)\n",
    "    pipe.fit(X_tr[v], resid_train[v])\n",
    "\n",
    "    # Tahmin: test artıkları\n",
    "    resid_pred = pd.Series(pipe.predict(X_te), index=X_te.index)\n",
    "    # Nihai tahmin = sNaive + tahmin edilen artık\n",
    "    final_fc = snaive_test.add(resid_pred, fill_value=0.0)\n",
    "\n",
    "    # Metrikler\n",
    "    m_snaive = metrics(y_test, snaive_test)\n",
    "    m_final  = metrics(y_test, final_fc)\n",
    "\n",
    "    rows.append({\n",
    "        \"target\": TARGET,\n",
    "        \"features\": [f\"{t}__lag{l}\" for t,l in exog_specs],\n",
    "        \"sNaive_MAE\": round(m_snaive[\"MAE\"],1), \"sNaive_RMSE\": round(m_snaive[\"RMSE\"],1), \"sNaive_MAPE%\": round(m_snaive[\"MAPE%\"],2),\n",
    "        \"ResCorr_MAE\": round(m_final[\"MAE\"],1), \"ResCorr_RMSE\": round(m_final[\"RMSE\"],1), \"ResCorr_MAPE%\": round(m_final[\"MAPE%\"],2),\n",
    "        \"improve_RMSE_%\": round((m_snaive[\"RMSE\"]-m_final[\"RMSE\"])/m_snaive[\"RMSE\"]*100,2)\n",
    "    })\n",
    "\n",
    "    # Kaydet: tahmin CSV\n",
    "    out_tbl = pd.DataFrame({\"actual\": y_test, \"sNaive\": snaive_test, \"residual_fc\": final_fc})\n",
    "    out_tbl.to_csv(OUT / f\"residual_forecast_{TARGET}.csv\")\n",
    "\n",
    "    # Sadece new_users için grafik çiz (rapora koyacağız)\n",
    "    if TARGET==\"new_users\":\n",
    "        plt.figure(figsize=(10,4))\n",
    "        plt.plot(pd.concat([y_train, y_test]).index, pd.concat([y_train, y_test]), label=\"gerçek\")\n",
    "        plt.plot(y_test.index, snaive_test, label=\"sNaive\")\n",
    "        plt.plot(y_test.index, final_fc, label=\"sNaive + residual-correction\")\n",
    "        plt.title(\"Residual-Correction Forecast — new_users (son 24 ay)\")\n",
    "        plt.xlabel(\"Tarih\"); plt.ylabel(\"new_users\"); plt.legend(loc=\"upper left\")\n",
    "        plt.tight_layout(); plt.savefig(OUT / \"residcorr_forecast_new_users.png\"); plt.close()\n",
    "\n",
    "res_tbl = pd.DataFrame(rows)\n",
    "res_tbl.to_csv(OUT / \"residual_correction_summary.csv\", index=False)\n",
    "\n",
    "print(\"Artık-düzeltme özet tablosu:\", OUT / \"residual_correction_summary.csv\")\n",
    "print(res_tbl.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5dec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R4: report.md'ye ek bölümleri yaz (STL + Residual-Correction)\n",
    "from pathlib import Path\n",
    "\n",
    "base = Path(\"/kaggle/working\")\n",
    "OUT  = base / \"report_assets\"\n",
    "\n",
    "append_md = f\"\"\"\n",
    "\n",
    "---\n",
    "\n",
    "## 7) STL Parçalama (Trend/Seasonal/Resid)\n",
    "Aşağıdaki görseller 12 aylık mevsimselliği ve uzun dönem eğilimi açıkça göstermektedir:\n",
    "- ![](report_assets/stl_new_users.png)\n",
    "- ![](report_assets/stl_new_projects.png)\n",
    "- ![](report_assets/stl_new_comments.png)\n",
    "\n",
    "## 8) sNaive + Artık-Düzeltme (operasyonel: lag≥1)\n",
    "- Özet tablo: `report_assets/residual_correction_summary.csv`\n",
    "- `new_users` görseli:  \n",
    "  ![](report_assets/residcorr_forecast_new_users.png)\n",
    "\n",
    "**Yorum:** Artık-düzeltme yaklaşımı, sNaive'e kıyasla seçilen hedef ve exog kombinasyonlarına göre { '{' }iyileşme veya benzer performans{ '}' } gösterebilir. Bu yöntem, sezonsallığın baskın olduğu serilerde “sNaive”in güçlü performansını bozmadan, ek sinyal oldukça hata azaltımı sağlar.\n",
    "\"\"\"\n",
    "\n",
    "p = base / \"report.md\"\n",
    "with p.open(\"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(append_md)\n",
    "\n",
    "print(\"Rapor güncellendi:\", p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f646e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R5: Rapor + varlıkları paketle (ZIP)\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "base = Path(\"/kaggle/working\")\n",
    "zip_path = base / \"report_bundle.zip\"\n",
    "\n",
    "with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as z:\n",
    "    z.write(base / \"report.md\", arcname=\"report.md\")\n",
    "    for p in (base / \"report_assets\").rglob(\"*\"):\n",
    "        if p.is_file():\n",
    "            z.write(p, arcname=f\"report_assets/{p.name}\")\n",
    "\n",
    "print(\"Hazır ZIP:\", zip_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85076ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PC-SUMMARY: platform_compare.csv -> 3 özet tablo\n",
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "ASSETS = Path(\"/kaggle/working/report_assets\")\n",
    "df = pd.read_csv(ASSETS / \"platform_compare.csv\")\n",
    "\n",
    "# Sayısal kolonları garanti altına al\n",
    "num_cols = [\"r0_pearson\",\"r0_spearman\",\"best_r_p\",\"best_r_s\",\"n0\",\"n_best\",\"best_lag_p\",\"best_lag_s\"]\n",
    "for c in num_cols:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "TOPN = 5  # her metrik için kaç terimi istiyorsun? (istersen 10 yap)\n",
    "\n",
    "# --- Özet A: Lag=0 (Spearman) en iyi TOPN — her metrik için\n",
    "lag0 = (df.sort_values([\"metric\",\"r0_spearman\"], ascending=[True,False])\n",
    "          .groupby(\"metric\").head(TOPN).copy())\n",
    "lag0[\"rank\"] = lag0.groupby(\"metric\")[\"r0_spearman\"].rank(ascending=False, method=\"first\").astype(int)\n",
    "lag0_out = lag0[[\"rank\",\"metric\",\"term\",\"r0_spearman\",\"r0_pearson\",\"n0\"]]\\\n",
    "            .sort_values([\"metric\",\"rank\"])\n",
    "\n",
    "# --- Özet B: En iyi lag (0..6) — Spearman — her metrik için TOPN\n",
    "best = (df.sort_values([\"metric\",\"best_r_s\"], ascending=[True,False])\n",
    "          .groupby(\"metric\").head(TOPN).copy())\n",
    "best[\"rank\"] = best.groupby(\"metric\")[\"best_r_s\"].rank(ascending=False, method=\"first\").astype(int)\n",
    "best_out = best[[\"rank\",\"metric\",\"term\",\"best_lag_s\",\"best_r_s\",\"n_best\"]]\\\n",
    "            .sort_values([\"metric\",\"rank\"])\n",
    "\n",
    "# --- Özet C: Genel görünüm — terim bazında\n",
    "overall = (df.groupby(\"term\")\n",
    "             .agg(metrics_covered=(\"metric\",\"nunique\"),\n",
    "                  r0_s_mean=(\"r0_spearman\",\"mean\"),\n",
    "                  r0_s_min=(\"r0_spearman\",\"min\"),\n",
    "                  r0_s_max=(\"r0_spearman\",\"max\"),\n",
    "                  best_s_mean=(\"best_r_s\",\"mean\"),\n",
    "                  lead_count=(\"best_lag_s\", lambda s: np.sum(pd.to_numeric(s, errors=\"coerce\") > 0)))\n",
    "             .reset_index()\n",
    "             .sort_values(\"r0_s_mean\", ascending=False))\n",
    "\n",
    "# Kaydet\n",
    "lag0_path   = ASSETS / \"platform_compare_top_lag0.csv\"\n",
    "best_path   = ASSETS / \"platform_compare_top_bestlag.csv\"\n",
    "overall_path= ASSETS / \"platform_compare_overall.csv\"\n",
    "\n",
    "lag0_out.to_csv(lag0_path, index=False)\n",
    "best_out.to_csv(best_path, index=False)\n",
    "overall.to_csv(overall_path, index=False)\n",
    "\n",
    "print(\"Kaydedildi:\")\n",
    "print(\"-\", lag0_path)\n",
    "print(\"-\", best_path)\n",
    "print(\"-\", overall_path)\n",
    "\n",
    "print(\"\\nÖrnek — Lag=0 TOPN (ilk satırlar):\")\n",
    "print(lag0_out.head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\nÖrnek — En iyi lag TOPN (ilk satırlar):\")\n",
    "print(best_out.head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\nÖrnek — Genel (ilk satırlar):\")\n",
    "print(overall.head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6eccf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOP GRANGER TABLOSU — Kaggle'da güzel görünüm + HTML çıktısı\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "path = Path(\"/kaggle/working/report_assets/top_granger.csv\")\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# (Opsiyonel) En düşük p-değerine göre sırala ve ilgilendiğimiz kolonları öne al\n",
    "preferred_cols = [c for c in [\"term\",\"metric\",\"best_lag\",\"best_p\",\"n\",\"sig(p<0.05)\",\"x_transform\",\"y_transform\"] if c in df.columns]\n",
    "if preferred_cols:\n",
    "    df = df[preferred_cols]\n",
    "df = df.sort_values(\"best_p\", na_position=\"last\").reset_index(drop=True)\n",
    "\n",
    "# Ekranda daha iyi görünmesi için bazı ayarlar\n",
    "pd.set_option(\"display.max_rows\", None)       # tüm satırları göster\n",
    "pd.set_option(\"display.max_colwidth\", None)   # uzun metinler kırpılmasın\n",
    "\n",
    "# p<0.05 hücrelerini yeşil renkle vurgula\n",
    "def highlight_sig(s):\n",
    "    return ['background-color: #d4edda' if (pd.notnull(v) and isinstance(v,(int,float)) and v < 0.05) else '' for v in s]\n",
    "\n",
    "styled = (df.style\n",
    "    .format({\"best_p\": \"{:.4f}\"})\n",
    "    .apply(highlight_sig, subset=[\"best_p\"])\n",
    "    .set_caption(\"Mevsimsel farkla Granger — en düşük p-değerleri (p<0.05 yeşil)\")\n",
    "    .hide(axis=\"index\")\n",
    ")\n",
    "\n",
    "# 1) Notebokta tabloyu göster (buradan screenshot alabilirsin)\n",
    "styled\n",
    "\n",
    "# 2) Aynı stilli görünümü HTML olarak da kaydet (dosyadan da ekran görüntüsü alabilirsin)\n",
    "html_out = Path(\"/kaggle/working/report_assets/top_granger_pretty.html\")\n",
    "html_out.write_text(styled.to_html(), encoding=\"utf-8\")\n",
    "print(\"Kaydedildi:\", html_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dd7ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODELS ALL TARGETS — Kaggle'da güzel tablo + HTML çıktısı\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "path = Path(\"/kaggle/working/report_assets/models_all_targets.csv\")\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# Kolon gruplarını otomatik bul (dosya yapısına dayanarak)\n",
    "rmse_cols = [c for c in df.columns if c.endswith(\"_RMSE\")]\n",
    "mae_cols  = [c for c in df.columns if c.endswith(\"_MAE\")]\n",
    "mape_cols = [c for c in df.columns if c.endswith(\"MAPE%\")]\n",
    "order = ([\"target\"] + rmse_cols + mae_cols + mape_cols + ([ \"SARIMAX_exog\" ] if \"SARIMAX_exog\" in df.columns else []))\n",
    "df = df[order]\n",
    "\n",
    "# Görsel ayarlar\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "# Biçimlendirme yardımcıları\n",
    "fmt_nums = {c: \"{:,.0f}\" for c in (rmse_cols + mae_cols)}\n",
    "fmt_nums.update({c: \"{:.2f}%\" for c in mape_cols})\n",
    "\n",
    "# NaN'ları \"—\" göster (özellikle SARIMAX NAN olabilir)\n",
    "df_display = df.copy()\n",
    "for c in rmse_cols + mae_cols + mape_cols:\n",
    "    df_display[c] = pd.to_numeric(df_display[c], errors=\"coerce\")\n",
    "\n",
    "styled = (\n",
    "    df_display.style\n",
    "      .format(fmt_nums, na_rep=\"—\")\n",
    "      # En düşük hataları yeşil vurgula (satır bazında)\n",
    "      .highlight_min(subset=rmse_cols, color=\"#d4edda\", axis=1)\n",
    "      .highlight_min(subset=mae_cols,  color=\"#d4edda\", axis=1)\n",
    "      .highlight_min(subset=mape_cols, color=\"#d4edda\", axis=1)\n",
    "      .set_caption(\"Üç Hedef İçin Model Karşılaştırması — En düşük hata yeşil\")\n",
    "      .hide(axis=\"index\")\n",
    ")\n",
    "\n",
    "# 1) Notebokta göster — buradan ekran görüntüsü alabilirsin\n",
    "styled\n",
    "\n",
    "# 2) Aynı stilli görünümü HTML olarak kaydet\n",
    "html_out = Path(\"/kaggle/working/report_assets/models_all_targets_pretty.html\")\n",
    "html_out.write_text(styled.to_html(), encoding=\"utf-8\")\n",
    "print(\"Kaydedildi:\", html_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23016e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LJUNG–BOX TABLOSU — Kaggle'da şık görünüm + HTML çıktısı\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "path = Path(\"/kaggle/working/report_assets/ljungbox_new_users.csv\")\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# Bazı kayıtlarda 'lag' kolonu CSV'ye yazılmaz; yoksa ekleyelim (12 ve 24 bekleniyor)\n",
    "if \"lag\" not in df.columns:\n",
    "    default_lags = [12, 24]\n",
    "    if len(df) == len(default_lags):\n",
    "        df.insert(0, \"lag\", default_lags)\n",
    "    else:\n",
    "        # emniyetli fallback: 1..n\n",
    "        df.insert(0, \"lag\", range(1, len(df) + 1))\n",
    "\n",
    "# Sayısal türleri garantiye al\n",
    "for c in [\"lag\", \"lb_stat\", \"lb_pvalue\"]:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "# Görsel ayarlar\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "# p>=0.05 (otokorelasyon YOK) yeşil; p<0.05 kırmızı\n",
    "def color_pvals(col):\n",
    "    out = []\n",
    "    for v in col:\n",
    "        if pd.isna(v):\n",
    "            out.append(\"\")\n",
    "        elif v >= 0.05:\n",
    "            out.append(\"background-color: #d4edda\")  # yeşil (iyi)\n",
    "        else:\n",
    "            out.append(\"background-color: #f8d7da\")  # kırmızı (kötü)\n",
    "    return out\n",
    "\n",
    "styled = (\n",
    "    df.style\n",
    "      .format({\"lb_stat\": \"{:.4f}\", \"lb_pvalue\": \"{:.4f}\"})\n",
    "      .apply(color_pvals, subset=[\"lb_pvalue\"])\n",
    "      .set_caption(\"Ljung–Box Test (new_users) — p≥0.05: otokorelasyon gözlenmedi\")\n",
    "      .hide(axis=\"index\")\n",
    ")\n",
    "\n",
    "# 1) Notebokta tabloyu göster — buradan ekran görüntüsü alabilirsin\n",
    "styled\n",
    "\n",
    "# 2) Aynı görünümü HTML olarak kaydet\n",
    "html_out = Path(\"/kaggle/working/report_assets/ljungbox_new_users_pretty.html\")\n",
    "html_out.write_text(styled.to_html(), encoding=\"utf-8\")\n",
    "print(\"Kaydedildi:\", html_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafee948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESIDUAL-CORRECTION ÖZETİ — Kaggle'da şık tablo + HTML çıktısı\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import ast\n",
    "\n",
    "path = Path(\"/kaggle/working/report_assets/residual_correction_summary.csv\")\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# (Opsiyonel) features sütununu okunur hale getir (liste gibi geldiyse)\n",
    "if \"features\" in df.columns:\n",
    "    def prettify_feats(x):\n",
    "        try:\n",
    "            v = ast.literal_eval(x) if isinstance(x, str) and x.strip().startswith((\"[\" , \"(\")) else x\n",
    "            if isinstance(v, (list, tuple)):\n",
    "                return \", \".join(map(str, v))\n",
    "            return str(v)\n",
    "        except Exception:\n",
    "            return str(x)\n",
    "    df[\"features\"] = df[\"features\"].apply(prettify_feats)\n",
    "\n",
    "# Sayısal kolonları biçimlendirme için güvenceye al\n",
    "num_cols = [c for c in df.columns if any(k in c for k in [\"MAE\",\"RMSE\",\"MAPE\",\"improve_RMSE_%\"])]\n",
    "for c in num_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "# Biçimlendirme şablonları\n",
    "fmt = {c: \"{:,.0f}\" for c in df.columns if c.endswith((\"MAE\",\"RMSE\"))}\n",
    "fmt.update({c: \"{:.2f}%\" for c in df.columns if c.endswith((\"MAPE%\",\"improve_RMSE_%\"))})\n",
    "\n",
    "# İyileşme yüzdesini renklendir (pozitif yeşil, negatif kırmızı)\n",
    "def color_improve(s):\n",
    "    out=[]\n",
    "    for v in s:\n",
    "        if pd.isna(v):\n",
    "            out.append(\"\")\n",
    "        elif v >= 0:\n",
    "            out.append(\"background-color: #d4edda\")  # yeşil\n",
    "        else:\n",
    "            out.append(\"background-color: #f8d7da\")  # kırmızı\n",
    "    return out\n",
    "\n",
    "# Satır içi en düşük hata (ResCorr vs sNaive) vurgusu için sütun listeleri\n",
    "rmse_cols = [c for c in df.columns if c.endswith(\"_RMSE\")]\n",
    "mae_cols  = [c for c in df.columns if c.endswith(\"_MAE\")]\n",
    "mape_cols = [c for c in df.columns if c.endswith(\"MAPE%\")]\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "styled = (\n",
    "    df.style\n",
    "      .format(fmt, na_rep=\"—\")\n",
    "      .apply(color_improve, subset=[\"improve_RMSE_%\"])            # iyileşme rengi\n",
    "      .highlight_min(subset=rmse_cols, color=\"#e2f0d9\", axis=1)   # en düşük RMSE açık yeşil\n",
    "      .highlight_min(subset=mae_cols,  color=\"#e2f0d9\", axis=1)   # en düşük MAE açık yeşil\n",
    "      .highlight_min(subset=mape_cols, color=\"#e2f0d9\", axis=1)   # en düşük MAPE açık yeşil\n",
    "      .set_caption(\"sNaive + Residual-Correction Özeti — improve_RMSE_% pozitifse (yeşil) daha iyi\")\n",
    "      .hide(axis=\"index\")\n",
    ")\n",
    "\n",
    "# 1) Notebokta tabloyu göster — buradan SS alabilirsin\n",
    "styled\n",
    "\n",
    "# 2) Aynı stilli görünümü HTML olarak kaydet\n",
    "html_out = Path(\"/kaggle/working/report_assets/residual_correction_summary_pretty.html\")\n",
    "html_out.write_text(styled.to_html(), encoding=\"utf-8\")\n",
    "print(\"Kaydedildi:\", html_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7c6f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPX-ALL-PRETTY — 3 appendix tablosunu şık göster + HTML kaydet\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "ASSETS = Path(\"/kaggle/working/report_assets\")\n",
    "\n",
    "def pretty_platform(csv_name, caption, sort_col, html_name):\n",
    "    df = pd.read_csv(ASSETS / csv_name)\n",
    "    # sayısalları güvenceye al\n",
    "    num_like = [\"r0_pearson\",\"r0_spearman\",\"best_r_p\",\"best_r_s\",\"n0\",\"n_best\",\"best_lag_p\",\"best_lag_s\"]\n",
    "    for c in df.columns:\n",
    "        if c in num_like:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    # sıralama (yüksek korelasyon üstte)\n",
    "    if sort_col in df.columns:\n",
    "        df = df.sort_values(sort_col, ascending=False)\n",
    "    # stil\n",
    "    grad_cols = [c for c in df.columns if c.startswith((\"r0_\",\"best_r_\"))]\n",
    "    styled = (\n",
    "        df.style\n",
    "          .format({c:\"{:.3f}\" for c in grad_cols})\n",
    "          .format({c:\"{:.0f}\" for c in df.columns if c in [\"n0\",\"n_best\",\"best_lag_p\",\"best_lag_s\"]})\n",
    "          .background_gradient(subset=grad_cols, cmap=\"Greens\")\n",
    "          .set_caption(caption)\n",
    "          .hide(axis=\"index\")\n",
    "    )\n",
    "    html_out = ASSETS / html_name\n",
    "    html_out.write_text(styled.to_html(), encoding=\"utf-8\")\n",
    "    print(\"Kaydedildi:\", html_out)\n",
    "    return styled\n",
    "\n",
    "# A) Platform Compare — Lag=0 (Spearman)\n",
    "styled_lag0 = pretty_platform(\n",
    "    \"platform_compare_top_lag0.csv\",\n",
    "    \"Platform Compare — Lag=0 (Spearman, en iyi TOPN)\",\n",
    "    \"r0_spearman\",\n",
    "    \"platform_compare_top_lag0_pretty.html\"\n",
    ")\n",
    "styled_lag0  # notebokta gösterilir (buradan SS al)\n",
    "\n",
    "# B) Platform Compare — En iyi lag 0..6 (Spearman)\n",
    "styled_best = pretty_platform(\n",
    "    \"platform_compare_top_bestlag.csv\",\n",
    "    \"Platform Compare — En iyi lag 0..6 (Spearman, en iyi TOPN)\",\n",
    "    \"best_r_s\",\n",
    "    \"platform_compare_top_bestlag_pretty.html\"\n",
    ")\n",
    "styled_best  # notebokta gösterilir (buradan SS al)\n",
    "\n",
    "# C) Residual-Correction Özeti (daha önce üretmediysen yeniden üretelim)\n",
    "def pretty_rescorr():\n",
    "    df = pd.read_csv(ASSETS / \"residual_correction_summary.csv\")\n",
    "    # sayısallar\n",
    "    for c in df.columns:\n",
    "        if any(k in c for k in [\"MAE\",\"RMSE\",\"MAPE\",\"improve_RMSE_%\"]):\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    fmt = {c:\"{:,.0f}\" for c in df.columns if c.endswith((\"MAE\",\"RMSE\"))}\n",
    "    fmt.update({c:\"{:.2f}%\" for c in df.columns if c.endswith((\"MAPE%\",\"improve_RMSE_%\"))})\n",
    "    def color_improve(col):\n",
    "        return [\"background-color: #d4edda\" if (pd.notnull(v) and v>=0) else (\"background-color: #f8d7da\" if pd.notnull(v) else \"\") for v in col]\n",
    "    rmse_cols = [c for c in df.columns if c.endswith(\"_RMSE\")]\n",
    "    mae_cols  = [c for c in df.columns if c.endswith(\"_MAE\")]\n",
    "    mape_cols = [c for c in df.columns if c.endswith(\"MAPE%\")]\n",
    "    styled = (\n",
    "        df.style\n",
    "          .format(fmt, na_rep=\"—\")\n",
    "          .apply(color_improve, subset=[\"improve_RMSE_%\"])\n",
    "          .highlight_min(subset=rmse_cols, color=\"#e2f0d9\", axis=1)\n",
    "          .highlight_min(subset=mae_cols,  color=\"#e2f0d9\", axis=1)\n",
    "          .highlight_min(subset=mape_cols, color=\"#e2f0d9\", axis=1)\n",
    "          .set_caption(\"sNaive + Residual-Correction Özeti — improve_RMSE_% pozitifse (yeşil) daha iyi\")\n",
    "          .hide(axis=\"index\")\n",
    "    )\n",
    "    html_out = ASSETS / \"residual_correction_summary_pretty.html\"\n",
    "    html_out.write_text(styled.to_html(), encoding=\"utf-8\")\n",
    "    print(\"Kaydedildi:\", html_out)\n",
    "    return styled\n",
    "\n",
    "styled_res = pretty_rescorr()\n",
    "styled_res  # notebokta gösterilir (buradan SS al)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3930d9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TÜM ÇIKTILARI ZIPLE: /kaggle/working -> /kaggle/working/all_outputs.zip\n",
    "import os, zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "BASE = Path(\"/kaggle/working\")\n",
    "ZIP_PATH = BASE / \"all_outputs.zip\"\n",
    "\n",
    "EXCLUDE_DIRS = {\".ipynb_checkpoints\", \"__pycache__\"}\n",
    "EXCLUDE_EXTS = {\".zip\"}  # mevcut zip'leri dahil etme\n",
    "\n",
    "def should_exclude(p: Path) -> bool:\n",
    "    if any(part in EXCLUDE_DIRS for part in p.parts):\n",
    "        return True\n",
    "    if p.suffix.lower() in EXCLUDE_EXTS:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "with zipfile.ZipFile(ZIP_PATH, \"w\", compression=zipfile.ZIP_DEFLATED) as z:\n",
    "    for root, dirs, files in os.walk(BASE):\n",
    "        # hariç klasörleri yürüyüşten çıkar\n",
    "        dirs[:] = [d for d in dirs if d not in EXCLUDE_DIRS]\n",
    "        for fname in files:\n",
    "            fpath = Path(root) / fname\n",
    "            if should_exclude(fpath) or fpath.samefile(ZIP_PATH):\n",
    "                continue\n",
    "            # arşiv içinde göreli yol kullan\n",
    "            arcname = fpath.relative_to(BASE)\n",
    "            z.write(fpath, arcname=str(arcname))\n",
    "\n",
    "print(\"Hazır ZIP:\", ZIP_PATH)\n",
    "print(\"Toplam dosya:\", len(zipfile.ZipFile(ZIP_PATH).namelist()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
